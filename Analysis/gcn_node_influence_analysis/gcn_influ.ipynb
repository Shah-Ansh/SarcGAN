{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "from matplotlib.cm import ScalarMappable\n",
    "from matplotlib.colors import Normalize\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.ticker as ticker\n",
    "from torch_geometric.utils import to_networkx\n",
    "from collections import defaultdict\n",
    "import matplotlib.patheffects as pe\n",
    "\n",
    "class GCNNodeVisualization(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    A wrapper for the SarcasmGCNLSTMDetector that captures intermediate GCN layer outputs\n",
    "    \"\"\"\n",
    "    def __init__(self, model):\n",
    "        super(GCNNodeVisualization, self).__init__()\n",
    "        self.model = model\n",
    "        self.gcn_outputs = []\n",
    "        self.hooks = []\n",
    "        \n",
    "        # Register forward hooks for each GCN layer\n",
    "        self._register_hooks()\n",
    "    \n",
    "    def _register_hooks(self):\n",
    "        def get_hook(layer_idx):\n",
    "            def hook(module, input, output):\n",
    "                self.gcn_outputs.append((layer_idx, output))\n",
    "            return hook\n",
    "        \n",
    "        # Register hooks for all GCN layers\n",
    "        # Assuming the model has gcn1, gcn2, gcn3, gcn4 as in your code\n",
    "        self.hooks.append(self.model.gcn1.register_forward_hook(get_hook(0)))\n",
    "        self.hooks.append(self.model.gcn2.register_forward_hook(get_hook(1)))\n",
    "        self.hooks.append(self.model.gcn3.register_forward_hook(get_hook(2)))\n",
    "        self.hooks.append(self.model.gcn4.register_forward_hook(get_hook(3)))\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, graph_x, graph_edge_index):\n",
    "        # Clear previous outputs\n",
    "        self.gcn_outputs = []\n",
    "        \n",
    "        # Forward pass through the model\n",
    "        output = self.model(input_ids, attention_mask, graph_x, graph_edge_index)\n",
    "        \n",
    "        # Sort outputs by layer index\n",
    "        self.gcn_outputs.sort(key=lambda x: x[0])\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def remove_hooks(self):\n",
    "        \"\"\"Remove all hooks to avoid memory leaks\"\"\"\n",
    "        for hook in self.hooks:\n",
    "            hook.remove()\n",
    "        self.hooks = []\n",
    "\n",
    "\n",
    "def should_exclude_token(token, format_tokens):\n",
    "    \"\"\"Check if a token should be excluded based on format tokens list\"\"\"\n",
    "    token_lower = token.lower()\n",
    "    return any(ft.lower() in token_lower for ft in format_tokens)\n",
    "\n",
    "\n",
    "def clean_token_for_display(token):\n",
    "    \"\"\"Clean up token for display by removing special markers\"\"\"\n",
    "    # Remove RoBERTa's special token marker\n",
    "    return token.replace('Ä ', '')\n",
    "\n",
    "\n",
    "def visualize_gcn_node_influence(model, tokenizer, comment, context='', model_path=None, \n",
    "                                device='cuda', save_path='gcn_node_visualization',\n",
    "                                format_tokens=None, layout_type='spring'):\n",
    "    \"\"\"\n",
    "    Visualize the influence of nodes in the GCN component of the sarcasm detection model\n",
    "    with improved label rendering and formatting\n",
    "    \n",
    "    Args:\n",
    "        model: The sarcasm detection model (SarcasmGCNLSTMDetector instance or None if model_path provided)\n",
    "        tokenizer: RoBERTa tokenizer\n",
    "        comment: The comment text to analyze\n",
    "        context: The context for the comment (optional)\n",
    "        model_path: Path to load model from (optional)\n",
    "        device: Device to run model on ('cuda' or 'cpu')\n",
    "        save_path: Base path to save visualization files\n",
    "        format_tokens: List of tokens to exclude from analysis (default: [\"comment\", \"context\", \":\", \" \"])\n",
    "        layout_type: Graph layout algorithm to use ('spring', 'kamada', 'spectral', 'circular')\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with GCN node influence data\n",
    "    \"\"\"\n",
    "    # Create directory for outputs\n",
    "    os.makedirs(os.path.dirname(save_path) if os.path.dirname(save_path) else '.', exist_ok=True)\n",
    "    \n",
    "    # Default format tokens to exclude if not provided\n",
    "    if format_tokens is None:\n",
    "        format_tokens = [\"comment\", \"context\", \":\", \" \"]\n",
    "    \n",
    "    # Load model if path provided\n",
    "    if model_path and model is None:\n",
    "        model = SarcasmGCNLSTMDetector().to(device)\n",
    "        model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    \n",
    "    # Wrap the model with our visualization wrapper\n",
    "    wrapped_model = GCNNodeVisualization(model).to(device)\n",
    "    wrapped_model.eval()\n",
    "    \n",
    "    # Format input\n",
    "    if isinstance(context, list):\n",
    "        context = \" \".join([str(c) for c in context if c])\n",
    "    \n",
    "    if context.strip():\n",
    "        combined_text = f\"Context: {context} Comment: {comment}\"\n",
    "    else:\n",
    "        combined_text = f\"Comment: {comment}\"\n",
    "    \n",
    "    # Create dataset for graph construction\n",
    "    dummy_df = pd.DataFrame({'comment': [comment], 'context': [context], 'label': [0]})\n",
    "    dataset = SarcasmGraphDataset(dummy_df, tokenizer)\n",
    "    \n",
    "    # Get the graph data\n",
    "    item = dataset[0]\n",
    "    graph_data = item['graph_data']\n",
    "    tokens = item['tokens']\n",
    "    \n",
    "    # Create a mask for tokens to include/exclude\n",
    "    token_mask = [not should_exclude_token(token, format_tokens) for token in tokens]\n",
    "    \n",
    "    # Prepare batch\n",
    "    batch = collate_batch([item])\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    graph_x = batch['graph_x'].to(device)\n",
    "    graph_edge_index = batch['graph_edge_index'].to(device)\n",
    "    \n",
    "    # Get prediction and GCN outputs\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            logits = wrapped_model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                graph_x=graph_x,\n",
    "                graph_edge_index=graph_edge_index\n",
    "            )\n",
    "            \n",
    "            prediction_prob = torch.sigmoid(logits).item()\n",
    "            prediction = \"Sarcastic\" if prediction_prob > 0.5 else \"Not Sarcastic\"\n",
    "            confidence = prediction_prob if prediction_prob > 0.5 else 1 - prediction_prob\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error during model prediction: {str(e)}\")\n",
    "            prediction = \"Unknown\"\n",
    "            confidence = 0.0\n",
    "    \n",
    "    print(f\"Model prediction: {prediction} (Confidence: {confidence:.4f})\")\n",
    "    \n",
    "    # Get the GCN outputs\n",
    "    gcn_outputs = wrapped_model.gcn_outputs\n",
    "    \n",
    "    # Remove hooks to prevent memory leaks\n",
    "    wrapped_model.remove_hooks()\n",
    "    \n",
    "    # Get the original graph structure using networkx\n",
    "    G = to_networkx(graph_data, to_undirected=True)\n",
    "    \n",
    "    # If no nodes, create a simple placeholder graph\n",
    "    if G.number_of_nodes() == 0:\n",
    "        print(\"Warning: Graph has no nodes. Creating a simple placeholder graph.\")\n",
    "        G = nx.Graph()\n",
    "        G.add_node(0, word=\"[No Valid Graph]\")\n",
    "    \n",
    "    # Add token text to graph nodes\n",
    "    for i, token in enumerate(tokens):\n",
    "        if i < len(G.nodes):\n",
    "            G.nodes[i]['word'] = token\n",
    "            G.nodes[i]['include'] = token_mask[i]\n",
    "    \n",
    "    # Calculate node importance at each layer\n",
    "    node_importance_by_layer = []\n",
    "    \n",
    "    for layer_idx, layer_output in gcn_outputs:\n",
    "        # Calculate node importance based on the norm of the feature vectors\n",
    "        node_features = layer_output.cpu().numpy()\n",
    "        node_importance = np.linalg.norm(node_features, axis=1)\n",
    "        node_importance_by_layer.append(node_importance)\n",
    "    \n",
    "    # Check if we have any node importance values\n",
    "    if not node_importance_by_layer:\n",
    "        print(\"Warning: No GCN outputs captured. Visualization will be limited.\")\n",
    "        # Create a simple visualization to indicate the issue\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.text(0.5, 0.5, \"No GCN outputs captured\", horizontalalignment='center',\n",
    "                verticalalignment='center', transform=plt.gca().transAxes, fontsize=14)\n",
    "        plt.axis('off')\n",
    "        plt.savefig(f\"{save_path}_no_outputs.png\", dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        # Return limited data\n",
    "        return {\n",
    "            'prediction': prediction,\n",
    "            'confidence': confidence,\n",
    "            'tokens': tokens,\n",
    "            'graph': G,\n",
    "            'error': 'No GCN outputs captured'\n",
    "        }\n",
    "    \n",
    "    # 1. Visualize node importance evolution across layers\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # Get filtered tokens and their indices\n",
    "    filtered_token_indices = [i for i, mask in enumerate(token_mask) if mask and i < len(tokens)]\n",
    "    filtered_tokens = [tokens[i] for i in filtered_token_indices]\n",
    "    \n",
    "    # Number of nodes to display (limit for readability)\n",
    "    num_nodes_to_display = min(10, len(filtered_token_indices))\n",
    "    \n",
    "    # Get indices of the most important nodes in the final layer (only considering filtered tokens)\n",
    "    if filtered_token_indices and len(node_importance_by_layer[-1]) > 0:\n",
    "        # Get importance values for filtered tokens\n",
    "        filtered_importance = [node_importance_by_layer[-1][i] if i < len(node_importance_by_layer[-1]) else 0 \n",
    "                             for i in filtered_token_indices]\n",
    "        \n",
    "        # Get top nodes among filtered tokens\n",
    "        top_filtered_indices = np.argsort(filtered_importance)[-num_nodes_to_display:]\n",
    "        top_nodes = [filtered_token_indices[i] for i in top_filtered_indices]\n",
    "        \n",
    "        # Track node importance across layers\n",
    "        for node_idx in top_nodes:\n",
    "            if node_idx < len(tokens):\n",
    "                display_token = clean_token_for_display(tokens[node_idx])\n",
    "                importance_values = [importance[node_idx] if node_idx < len(importance) else 0 \n",
    "                                  for importance in node_importance_by_layer]\n",
    "                plt.plot(range(1, len(gcn_outputs) + 1), importance_values, marker='o', \n",
    "                       label=f\"{display_token} (Node {node_idx})\")\n",
    "    \n",
    "    plt.title('Node Importance Evolution Across GCN Layers')\n",
    "    plt.xlabel('GCN Layer')\n",
    "    plt.ylabel('Node Importance (L2 Norm)')\n",
    "    plt.xticks(range(1, len(gcn_outputs) + 1))\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.legend(loc='best')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{save_path}_evolution.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # 2. Create graph visualizations for each layer\n",
    "    for layer_idx, node_importance in enumerate(node_importance_by_layer):\n",
    "        fig, ax = plt.subplots(figsize=(12, 12))\n",
    "        \n",
    "        # Create a copy of the graph\n",
    "        G_layer = G.copy()\n",
    "        \n",
    "        # Add node importance to the graph\n",
    "        for i, importance in enumerate(node_importance):\n",
    "            if i < len(G_layer.nodes):\n",
    "                G_layer.nodes[i]['importance'] = float(importance)\n",
    "                \n",
    "        # Create a filtered graph with only the included nodes\n",
    "        G_filtered = nx.Graph()\n",
    "        \n",
    "        # Add only the included nodes to the filtered graph\n",
    "        node_mapping = {}  # Maps original node IDs to new ones\n",
    "        new_id = 0\n",
    "        \n",
    "        for node in G_layer.nodes:\n",
    "            if G_layer.nodes[node].get('include', True):\n",
    "                # Copy node attributes\n",
    "                attrs = {k: v for k, v in G_layer.nodes[node].items()}\n",
    "                G_filtered.add_node(new_id, **attrs)\n",
    "                node_mapping[node] = new_id\n",
    "                new_id += 1\n",
    "        \n",
    "        # Add edges between included nodes\n",
    "        for u, v in G_layer.edges:\n",
    "            if G_layer.nodes[u].get('include', True) and G_layer.nodes[v].get('include', True):\n",
    "                G_filtered.add_edge(node_mapping[u], node_mapping[v])\n",
    "        \n",
    "        # If we have nodes in the filtered graph\n",
    "        if G_filtered.number_of_nodes() > 0:\n",
    "            # Get importance values for included nodes\n",
    "            included_importance = [G_filtered.nodes[n].get('importance', 0) for n in G_filtered.nodes]\n",
    "            max_importance = max(included_importance) if included_importance else 1.0\n",
    "            \n",
    "            # Set node sizes proportional to importance\n",
    "            node_sizes = [1000 * (G_filtered.nodes[n].get('importance', 0) / max_importance if max_importance > 0 else 0) \n",
    "                         for n in G_filtered.nodes]\n",
    "            node_colors = [G_filtered.nodes[n].get('importance', 0) for n in G_filtered.nodes]\n",
    "            \n",
    "            # Choose layout algorithm\n",
    "            if layout_type == 'kamada':\n",
    "                pos = nx.kamada_kawai_layout(G_filtered)\n",
    "            elif layout_type == 'spectral':\n",
    "                pos = nx.spectral_layout(G_filtered)\n",
    "            elif layout_type == 'circular':\n",
    "                pos = nx.circular_layout(G_filtered)\n",
    "            else:  # default spring layout\n",
    "                pos = nx.spring_layout(G_filtered, k=0.9, iterations=100, seed=42)\n",
    "            \n",
    "            # Draw graph\n",
    "            nx.draw_networkx_edges(G_filtered, pos, alpha=0.3, width=1.0, ax=ax)\n",
    "            \n",
    "            nodes = nx.draw_networkx_nodes(G_filtered, pos, node_size=node_sizes, \n",
    "                                          node_color=node_colors, cmap=plt.cm.viridis, \n",
    "                                          alpha=0.8, ax=ax)\n",
    "            \n",
    "            # Add colorbar\n",
    "            sm = ScalarMappable(cmap=plt.cm.viridis, norm=Normalize(vmin=0, vmax=max_importance))\n",
    "            sm.set_array([])\n",
    "            cbar = fig.colorbar(sm, ax=ax)\n",
    "            cbar.set_label('Node Importance')\n",
    "            \n",
    "            # Create a mapping from original tokens to display tokens\n",
    "            reverse_mapping = {v: k for k, v in node_mapping.items()}\n",
    "            token_display = {}\n",
    "            \n",
    "            for new_id, orig_id in reverse_mapping.items():\n",
    "                if orig_id < len(tokens):\n",
    "                    token_display[new_id] = clean_token_for_display(tokens[orig_id])\n",
    "            \n",
    "            # Improved label placement\n",
    "            label_positions = {}\n",
    "            \n",
    "            # First pass - get initial positions\n",
    "            for node in G_filtered.nodes:\n",
    "                x, y = pos[node]\n",
    "                \n",
    "                # Calculate node radius based on size\n",
    "                node_size = node_sizes[list(G_filtered.nodes).index(node)]\n",
    "                node_radius = np.sqrt(node_size / np.pi)\n",
    "                \n",
    "                # Set base offset based on node size\n",
    "                base_offset = max(0.1, node_radius / 2000)\n",
    "                \n",
    "                # Calculate initial position based on quadrant\n",
    "                if x >= 0 and y >= 0:  # Top right quadrant\n",
    "                    offset_x, offset_y = base_offset, base_offset\n",
    "                elif x < 0 and y >= 0:  # Top left quadrant\n",
    "                    offset_x, offset_y = -base_offset, base_offset\n",
    "                elif x >= 0 and y < 0:  # Bottom right quadrant\n",
    "                    offset_x, offset_y = base_offset, -base_offset\n",
    "                else:  # Bottom left quadrant\n",
    "                    offset_x, offset_y = -base_offset, -base_offset\n",
    "                \n",
    "                # Initial label position\n",
    "                label_positions[node] = (x + offset_x, y + offset_y)\n",
    "            \n",
    "            # Second pass - apply force-directed label positioning\n",
    "            for _ in range(10):  # Number of iterations for force-directed adjustment\n",
    "                for node in label_positions:\n",
    "                    x, y = label_positions[node]\n",
    "                    fx, fy = 0, 0  # Force components\n",
    "                    \n",
    "                    # Repulsive forces from other labels\n",
    "                    for other_node, (other_x, other_y) in label_positions.items():\n",
    "                        if other_node != node:\n",
    "                            dx = x - other_x\n",
    "                            dy = y - other_y\n",
    "                            dist = max(0.01, np.sqrt(dx*dx + dy*dy))  # Avoid division by zero\n",
    "                            \n",
    "                            # Repulsive force inversely proportional to distance\n",
    "                            if dist < 0.2:  # Only apply repulsion for close labels\n",
    "                                force = 0.001 / (dist * dist)\n",
    "                                fx += force * dx / dist\n",
    "                                fy += force * dy / dist\n",
    "                    \n",
    "                    # Attractive force to original node position\n",
    "                    node_x, node_y = pos[node]\n",
    "                    dx = node_x - x\n",
    "                    dy = node_y - y\n",
    "                    dist = max(0.01, np.sqrt(dx*dx + dy*dy))\n",
    "                    \n",
    "                    # Attractive force proportional to distance but with a maximum\n",
    "                    if dist > 0.3:  # Only pull back if too far\n",
    "                        force = 0.1 * dist\n",
    "                        fx += force * dx / dist\n",
    "                        fy += force * dy / dist\n",
    "                    \n",
    "                    # Update position\n",
    "                    label_positions[node] = (x + 0.1 * fx, y + 0.1 * fy)\n",
    "            \n",
    "            # Add labels at final positions\n",
    "            for node in label_positions:\n",
    "                if node in token_display:\n",
    "                    display_token = token_display[node]\n",
    "                    label_x, label_y = label_positions[node]\n",
    "                    \n",
    "                    # Add label with enhanced visibility\n",
    "                    text = ax.text(label_x, label_y, display_token, \n",
    "                                  fontsize=11, ha='center', va='center', weight='bold',\n",
    "                                  bbox=dict(boxstyle=\"round\", fc=\"white\", ec=\"black\", alpha=0.9, pad=0.3))\n",
    "                    text.set_path_effects([pe.withStroke(linewidth=2, foreground='white')])\n",
    "        else:\n",
    "            # No nodes in filtered graph\n",
    "            ax.text(0.5, 0.5, \"No nodes remain after filtering\", \n",
    "                   horizontalalignment='center', verticalalignment='center', \n",
    "                   transform=ax.transAxes, fontsize=12)\n",
    "        \n",
    "        ax.set_title(f'Node Importance in GCN Layer {layer_idx + 1}', pad=20)\n",
    "        ax.axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{save_path}_layer{layer_idx + 1}.png\", dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    \n",
    "    # 3. Create a final visualization with all important connections\n",
    "    fig, ax = plt.subplots(figsize=(15, 15))\n",
    "    \n",
    "    # Get the final layer importance\n",
    "    final_importance = node_importance_by_layer[-1]\n",
    "    \n",
    "    # Create a copy of the graph\n",
    "    G_final = G.copy()\n",
    "    \n",
    "    # Add node importance to the graph\n",
    "    for i, importance in enumerate(final_importance):\n",
    "        if i < len(G_final.nodes):\n",
    "            G_final.nodes[i]['importance'] = float(importance)\n",
    "    \n",
    "    # Create a filtered graph with only the included nodes\n",
    "    G_filtered = nx.Graph()\n",
    "    \n",
    "    # Add only the included nodes to the filtered graph\n",
    "    node_mapping = {}  # Maps original node IDs to new ones\n",
    "    new_id = 0\n",
    "    \n",
    "    for node in G_final.nodes:\n",
    "        if G_final.nodes[node].get('include', True):\n",
    "            # Copy node attributes\n",
    "            attrs = {k: v for k, v in G_final.nodes[node].items()}\n",
    "            G_filtered.add_node(new_id, **attrs)\n",
    "            node_mapping[node] = new_id\n",
    "            new_id += 1\n",
    "    \n",
    "    # Add edges between included nodes\n",
    "    for u, v in G_final.edges:\n",
    "        if G_final.nodes[u].get('include', True) and G_final.nodes[v].get('include', True):\n",
    "            G_filtered.add_edge(node_mapping[u], node_mapping[v])\n",
    "    \n",
    "    # Choose layout algorithm for filtered graph\n",
    "    if layout_type == 'kamada':\n",
    "        pos = nx.kamada_kawai_layout(G_filtered)\n",
    "    elif layout_type == 'spectral':\n",
    "        pos = nx.spectral_layout(G_filtered)\n",
    "    elif layout_type == 'circular':\n",
    "        pos = nx.circular_layout(G_filtered)\n",
    "    else:  # default spring layout\n",
    "        pos = nx.spring_layout(G_filtered, k=1.2, iterations=200, seed=42)\n",
    "    \n",
    "    # If we have nodes in the filtered graph\n",
    "    if G_filtered.number_of_nodes() > 0:\n",
    "        # Get importance values for included nodes\n",
    "        included_importance = [G_filtered.nodes[n].get('importance', 0) for n in G_filtered.nodes]\n",
    "        max_importance = max(included_importance) if included_importance else 1.0\n",
    "        \n",
    "        # Calculate edge weights based on node importance\n",
    "        edge_weights = []\n",
    "        for u, v in G_filtered.edges:\n",
    "            weight = (G_filtered.nodes[u].get('importance', 0) + G_filtered.nodes[v].get('importance', 0)) / 2.0\n",
    "            G_filtered[u][v]['weight'] = weight\n",
    "            edge_weights.append(weight)\n",
    "        \n",
    "        # Normalize edge weights for visualization\n",
    "        if edge_weights:\n",
    "            max_edge_weight = max(edge_weights)\n",
    "            edge_widths = [3.0 * G_filtered[u][v].get('weight', 0) / max_edge_weight if max_edge_weight > 0 else 1.0 \n",
    "                          for u, v in G_filtered.edges]\n",
    "        else:\n",
    "            edge_widths = [1.0]\n",
    "        \n",
    "        # Set node sizes based on importance\n",
    "        node_sizes = [2000 * (G_filtered.nodes[n].get('importance', 0) / max_importance) if max_importance > 0 else 500 \n",
    "                     for n in G_filtered.nodes]\n",
    "        node_colors = [G_filtered.nodes[n].get('importance', 0) for n in G_filtered.nodes]\n",
    "        \n",
    "        # Draw graph - first edges\n",
    "        nx.draw_networkx_edges(G_filtered, pos, alpha=0.5, width=edge_widths, ax=ax)\n",
    "        \n",
    "        # Then nodes\n",
    "        nodes = nx.draw_networkx_nodes(G_filtered, pos, node_size=node_sizes, \n",
    "                                      node_color=node_colors, cmap=plt.cm.plasma, \n",
    "                                      alpha=0.9, ax=ax)\n",
    "        \n",
    "        # Add colorbar\n",
    "        sm = ScalarMappable(cmap=plt.cm.plasma, norm=Normalize(vmin=0, vmax=max_importance))\n",
    "        sm.set_array([])\n",
    "        cbar = fig.colorbar(sm, ax=ax)\n",
    "        cbar.set_label('Node Importance (Final Layer)')\n",
    "        \n",
    "        # Create a mapping from original tokens to display tokens\n",
    "        reverse_mapping = {v: k for k, v in node_mapping.items()}\n",
    "        token_display = {}\n",
    "        \n",
    "        for new_id, orig_id in reverse_mapping.items():\n",
    "            if orig_id < len(tokens):\n",
    "                token_display[new_id] = clean_token_for_display(tokens[orig_id])\n",
    "        \n",
    "        # Calculate node radii for node-aware label placement\n",
    "        node_radii = {}\n",
    "        for i, n in enumerate(G_filtered.nodes):\n",
    "            node_radii[n] = np.sqrt(node_sizes[i] / np.pi) / 100\n",
    "        \n",
    "        # For each node, place label at an optimal position\n",
    "        label_positions = {}\n",
    "        \n",
    "        for node in G_filtered.nodes:\n",
    "            if node in token_display:\n",
    "                x, y = pos[node]\n",
    "                \n",
    "                # Use node size to determine offset distance\n",
    "                importance = G_filtered.nodes[node].get('importance', 0)\n",
    "                relative_size = importance / max_importance if max_importance > 0 else 0.5\n",
    "                \n",
    "                # Find best angle for label\n",
    "                best_angle = 0\n",
    "                min_overlap = float('inf')\n",
    "                \n",
    "                for angle in np.linspace(0, 2*np.pi, 16, endpoint=False):\n",
    "                    # Test position at this angle\n",
    "                    offset = 0.15 + relative_size * 0.05  # Scale offset with node size\n",
    "                    test_x = x + offset * np.cos(angle)\n",
    "                    test_y = y + offset * np.sin(angle)\n",
    "                    \n",
    "                    # Check for overlaps with other nodes and labels\n",
    "                    overlap_score = 0\n",
    "                    \n",
    "                    # Check distance to other nodes\n",
    "                    for other_node in G_filtered.nodes:\n",
    "                        if other_node != node:\n",
    "                            other_x, other_y = pos[other_node]\n",
    "                            dist = np.sqrt((test_x - other_x)**2 + (test_y - other_y)**2)\n",
    "                            \n",
    "                            # Add penalty for being close to other nodes\n",
    "                            if dist < 0.2:\n",
    "                                overlap_score += (0.2 - dist) * 5\n",
    "                    \n",
    "                    # Check distance to existing labels\n",
    "                    for other_node, (other_x, other_y) in label_positions.items():\n",
    "                        dist = np.sqrt((test_x - other_x)**2 + (test_y - other_y)**2)\n",
    "                        \n",
    "                        # Add penalty for being close to other labels\n",
    "                        if dist < 0.2:\n",
    "                            overlap_score += (0.2 - dist) * 10\n",
    "                    \n",
    "                    if overlap_score < min_overlap:\n",
    "                        min_overlap = overlap_score\n",
    "                        best_angle = angle\n",
    "                \n",
    "                # Place label at best angle with offset proportional to node size\n",
    "                offset = 0.15 + relative_size * 0.05\n",
    "                label_x = x + offset * np.cos(best_angle)\n",
    "                label_y = y + offset * np.sin(best_angle)\n",
    "                \n",
    "                # Save position\n",
    "                label_positions[node] = (label_x, label_y)\n",
    "        \n",
    "        # Add node labels and connecting lines\n",
    "        for node, (label_x, label_y) in label_positions.items():\n",
    "            if node in token_display:\n",
    "                display_token = token_display[node]\n",
    "                node_x, node_y = pos[node]\n",
    "                \n",
    "                # Draw thin connecting line\n",
    "                ax.plot([node_x, label_x], [node_y, label_y], 'k-', alpha=0.3, linewidth=0.5)\n",
    "                \n",
    "                # Add text label\n",
    "                text = ax.text(label_x, label_y, display_token, \n",
    "                             fontsize=12, ha='center', va='center', weight='bold',\n",
    "                             bbox=dict(boxstyle=\"round\", fc=\"white\", ec=\"black\", alpha=0.9, pad=0.4))\n",
    "                text.set_path_effects([pe.withStroke(linewidth=2, foreground='white')])\n",
    "    else:\n",
    "        # No nodes in filtered graph\n",
    "        ax.text(0.5, 0.5, \"No nodes remain after filtering\", \n",
    "               horizontalalignment='center', verticalalignment='center', \n",
    "               transform=ax.transAxes, fontsize=14)\n",
    "    \n",
    "    ax.set_title('Final GCN Layer Node Importance and Connections', fontsize=16, pad=20)\n",
    "    ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{save_path}_final.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # 4. Create a detailed analysis table - only for included tokens\n",
    "    node_analysis = []\n",
    "    \n",
    "    for i, token in enumerate(tokens):\n",
    "        if i < len(final_importance) and token_mask[i]:  # Only include filtered tokens\n",
    "            analysis = {\n",
    "                'node_idx': i,\n",
    "                'token': token,\n",
    "                'display_token': clean_token_for_display(token),\n",
    "                'final_importance': float(final_importance[i]),\n",
    "                'layer_evolution': [float(layer[i]) if i < len(layer) else 0.0 for layer in node_importance_by_layer],\n",
    "                'growth_rate': float(final_importance[i] / node_importance_by_layer[0][i]) if i < len(node_importance_by_layer[0]) and node_importance_by_layer[0][i] > 0 else 0\n",
    "            }\n",
    "            node_analysis.append(analysis)\n",
    "    \n",
    "    # Sort by final importance\n",
    "    node_analysis = sorted(node_analysis, key=lambda x: x['final_importance'], reverse=True)\n",
    "    \n",
    "    # Save analysis to CSV\n",
    "    analysis_df = pd.DataFrame(node_analysis)\n",
    "    analysis_df.to_csv(f\"{save_path}_analysis.csv\", index=False)\n",
    "    \n",
    "    # 5. Calculate aggregated metrics for semantic analysis - only include filtered tokens\n",
    "    aggregated_importance = defaultdict(float)\n",
    "    token_count = defaultdict(int)\n",
    "    \n",
    "    # Aggregate importance by token\n",
    "    for analysis in node_analysis:\n",
    "        display_token = analysis['display_token']\n",
    "        importance = analysis['final_importance']\n",
    "        aggregated_importance[display_token] += importance\n",
    "        token_count[display_token] += 1\n",
    "    \n",
    "    # Calculate average importance per token\n",
    "    avg_importance = {token: importance / token_count[token] for token, importance in aggregated_importance.items()}\n",
    "    \n",
    "    # Sort tokens by average importance\n",
    "    sorted_tokens = sorted(avg_importance.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Create token importance visualization\n",
    "    if sorted_tokens:\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        # Number of tokens to show\n",
    "        n_tokens = min(15, len(sorted_tokens))\n",
    "        \n",
    "        tokens_to_plot = [t[0] for t in sorted_tokens[:n_tokens]]\n",
    "        importances_to_plot = [t[1] for t in sorted_tokens[:n_tokens]]\n",
    "        \n",
    "        plt.barh(range(n_tokens), importances_to_plot, color='coral')\n",
    "        plt.yticks(range(n_tokens), tokens_to_plot)\n",
    "        plt.title('Most Important Tokens in Sarcasm Detection (Final GCN Layer)')\n",
    "        plt.xlabel('Average Node Importance')\n",
    "        plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{save_path}_token_importance.png\", dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    else:\n",
    "        print(\"Warning: No token importance data available for visualization\")\n",
    "    \n",
    "    # 6. Create a summary report\n",
    "    try:\n",
    "        # Generate a markdown report\n",
    "        report = f\"\"\"\n",
    "# GCN Node Influence Analysis for Sarcasm Detection\n",
    "\n",
    "## Overview\n",
    "- **Text**: \"{comment}\"\n",
    "- **Context**: \"{context if context else 'None'}\"\n",
    "- **Prediction**: {prediction} (Confidence: {confidence:.4f})\n",
    "- **Number of Nodes**: {len(node_analysis)} (after filtering)\n",
    "- **Tokens Analyzed**: {len(node_analysis)}\n",
    "\n",
    "## Key Findings\n",
    "\n",
    "### Most Important Nodes\n",
    "The following tokens have the highest importance in the final GCN layer:\n",
    "        \n",
    "| Node | Token | Importance | Growth Rate |\n",
    "|------|-------|------------|-------------|\n",
    "\"\"\"\n",
    "        \n",
    "        for i, analysis in enumerate(node_analysis[:5]):  # Top 5 nodes\n",
    "            report += f\"| {analysis['node_idx']} | {analysis['display_token']} | {analysis['final_importance']:.4f} | {analysis['growth_rate']:.2f}x |\\n\"\n",
    "        \n",
    "        if node_analysis:\n",
    "            report += \"\"\"\n",
    "### Node Importance Evolution\n",
    "The following shows how node importance evolves across GCN layers:\n",
    "\n",
    "| Node | Token | Layer 1 | Layer 2 | Layer 3 | Layer 4 | Growth Pattern |\n",
    "|------|-------|---------|---------|---------|---------|----------------|\n",
    "\"\"\"\n",
    "            \n",
    "            for i, analysis in enumerate(node_analysis[:5]):  # Top 5 nodes\n",
    "                layer_values = analysis['layer_evolution']\n",
    "                \n",
    "                if len(layer_values) >= 3:\n",
    "                    pattern = \"Increasing\" if layer_values[-1] > layer_values[0] else \"Decreasing\"\n",
    "                    if layer_values[1] > layer_values[0] and layer_values[2] < layer_values[1]:\n",
    "                        pattern = \"Peak at Layer 2\"\n",
    "                    elif layer_values[1] < layer_values[0] and layer_values[2] > layer_values[1]:\n",
    "                        pattern = \"Dip at Layer 2\"\n",
    "                    \n",
    "                    # Access layer values safely\n",
    "                    layer1 = layer_values[0] if 0 < len(layer_values) else 0.0\n",
    "                    layer2 = layer_values[1] if 1 < len(layer_values) else 0.0\n",
    "                    layer3 = layer_values[2] if 2 < len(layer_values) else 0.0\n",
    "                    layer4 = layer_values[3] if 3 < len(layer_values) else 0.0\n",
    "                    \n",
    "                    report += f\"| {analysis['node_idx']} | {analysis['display_token']} | {layer1:.4f} | {layer2:.4f} | {layer3:.4f} | {layer4:.4f} | {pattern} |\\n\"\n",
    "        else:\n",
    "            report += \"\\nNo significant node importance data available for analysis.\"\n",
    "        \n",
    "        # Save report\n",
    "        with open(f\"{save_path}_report.md\", 'w') as f:\n",
    "            f.write(report)\n",
    "        \n",
    "        print(f\"Full GCN node analysis report saved to {save_path}_report.md\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error generating report: {str(e)}\")\n",
    "    \n",
    "    # Return analysis data\n",
    "    return {\n",
    "        'prediction': prediction,\n",
    "        'confidence': confidence,\n",
    "        'tokens': [clean_token_for_display(tokens[i]) for i in range(len(tokens)) if token_mask[i]],  # Only return filtered tokens\n",
    "        'node_analysis': node_analysis,\n",
    "        'node_importance_by_layer': node_importance_by_layer,\n",
    "        'graph': G_filtered  # Return the filtered graph\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GloVe embeddings...\n",
      "Loaded GloVe embeddings with dimension: 300\n",
      "Loaded spaCy model successfully\n",
      "Loaded SenticNet successfully\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    RobertaTokenizer,\n",
    "    RobertaModel,\n",
    "    # AdamW,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "from torch.optim import AdamW\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "import ast\n",
    "import os\n",
    "import gc\n",
    "import networkx as nx\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import from_networkx\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.utils import resample\n",
    "import spacy\n",
    "from senticnet.senticnet import SenticNet\n",
    "import gensim.downloader as gensim_downloader\n",
    "\n",
    "print(\"Loading GloVe embeddings...\")\n",
    "try:\n",
    "    glove_embeddings = gensim_downloader.load(\"glove-wiki-gigaword-300\")\n",
    "    EMBEDDING_DIM = 300\n",
    "    print(f\"Loaded GloVe embeddings with dimension: {EMBEDDING_DIM}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading GloVe embeddings: {str(e)}\")\n",
    "    print(\"Using random embeddings instead\")\n",
    "    glove_embeddings = None\n",
    "    EMBEDDING_DIM = 300\n",
    "\n",
    "# Load spaCy model\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    print(\"Loaded spaCy model successfully\")\n",
    "except:\n",
    "    print(\"Downloading spaCy model...\")\n",
    "    import subprocess\n",
    "\n",
    "    subprocess.run([\"python\", \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"])\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Initialize SenticNet\n",
    "try:\n",
    "    sn = SenticNet()\n",
    "    print(\"Loaded SenticNet successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading SenticNet: {str(e)}\")\n",
    "    sn = None\n",
    "\n",
    "class SarcasmGraphDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length=128):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.comments = df[\"comment\"].values\n",
    "        self.contexts = df[\"context\"].values\n",
    "        self.labels = df[\"label\"].values\n",
    "        self.max_length = max_length\n",
    "        self.window_size = 2  # Window size for graph construction\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.comments)\n",
    "\n",
    "    def get_embedding(self, word):\n",
    "        \"\"\"Get the GloVe embedding for a word\"\"\"\n",
    "        word = word.lower()\n",
    "        if glove_embeddings and word in glove_embeddings:\n",
    "            return torch.tensor(glove_embeddings[word], dtype=torch.float)\n",
    "        else:\n",
    "            # Use random embedding if word not found\n",
    "            return torch.randn(EMBEDDING_DIM, dtype=torch.float)\n",
    "\n",
    "    def get_sentiment_features(self, word):\n",
    "        \"\"\"Extract sentiment features using SenticNet\"\"\"\n",
    "        try:\n",
    "            if sn is not None:\n",
    "                concept_info = sn.concept(word)\n",
    "                # Extract polarity value (float between -1 and 1)\n",
    "                polarity = float(concept_info[\"polarity_value\"])\n",
    "                # Create a 5-dimensional feature: [polarity, is_positive, is_negative, is_neutral, intensity]\n",
    "                is_positive = 1.0 if polarity > 0.1 else 0.0\n",
    "                is_negative = 1.0 if polarity < -0.1 else 0.0\n",
    "                is_neutral = 1.0 if abs(polarity) <= 0.1 else 0.0\n",
    "                intensity = abs(polarity)\n",
    "                return torch.tensor(\n",
    "                    [polarity, is_positive, is_negative, is_neutral, intensity],\n",
    "                    dtype=torch.float,\n",
    "                )\n",
    "            else:\n",
    "                return torch.zeros(5, dtype=torch.float)\n",
    "        except:\n",
    "            # Word not found in SenticNet\n",
    "            return torch.zeros(5, dtype=torch.float)\n",
    "\n",
    "    def create_graph_from_text(self, text):\n",
    "        \"\"\"Create a graph representation of text for GCN with enhanced features\"\"\"\n",
    "        # Parse text with spaCy for dependency parsing\n",
    "        doc = nlp(text.lower())\n",
    "\n",
    "        # Create a graph where nodes are tokens\n",
    "        G = nx.Graph()\n",
    "\n",
    "        # Store tokens for later embedding lookup\n",
    "        tokens = [token.text for token in doc]\n",
    "\n",
    "        # Add nodes with positions\n",
    "        for i, token in enumerate(doc):\n",
    "            G.add_node(i, word=token.text, pos=token.pos_)\n",
    "\n",
    "        # Add edges based on window and dependencies\n",
    "        # 1. Window-based edges\n",
    "        for i in range(len(tokens)):\n",
    "            for j in range(i + 1, min(i + self.window_size + 1, len(tokens))):\n",
    "                G.add_edge(i, j, edge_type=0)  # Type 0: window edge\n",
    "\n",
    "        # 2. Dependency-based edges\n",
    "        for token in doc:\n",
    "            if token.i < len(tokens) and token.head.i < len(tokens):\n",
    "                G.add_edge(\n",
    "                    token.i, token.head.i, edge_type=1\n",
    "                )  # Type 1: dependency edge\n",
    "\n",
    "        # Convert to PyTorch Geometric Data object\n",
    "        if len(G.nodes) > 0:\n",
    "            data = from_networkx(G)\n",
    "\n",
    "            # Create feature matrix for nodes [GloVe (25d) + Sentiment (5d)]\n",
    "            feature_dim = EMBEDDING_DIM + 5\n",
    "            features = torch.zeros((len(G.nodes), feature_dim), dtype=torch.float)\n",
    "\n",
    "            for i, token_text in enumerate(tokens):\n",
    "                if i < len(features):\n",
    "                    # GloVe embedding\n",
    "                    glove_feature = self.get_embedding(token_text)\n",
    "                    # Sentiment features\n",
    "                    sentiment_feature = self.get_sentiment_features(token_text)\n",
    "                    # Concatenate features\n",
    "                    if (\n",
    "                        len(glove_feature) == EMBEDDING_DIM\n",
    "                        and len(sentiment_feature) == 5\n",
    "                    ):\n",
    "                        features[i] = torch.cat([glove_feature, sentiment_feature])\n",
    "\n",
    "            data.x = features\n",
    "            return data, tokens\n",
    "        else:\n",
    "            # Return empty graph if there are no nodes\n",
    "            empty_data = Data(\n",
    "                x=torch.zeros((1, feature_dim), dtype=torch.float),\n",
    "                edge_index=torch.zeros((2, 0), dtype=torch.long),\n",
    "            )\n",
    "            return empty_data, []\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        comment = str(self.comments[idx])\n",
    "\n",
    "        # Parse context if it's a string\n",
    "        if isinstance(self.contexts[idx], str):\n",
    "            try:\n",
    "                context_list = ast.literal_eval(self.contexts[idx])\n",
    "            except:\n",
    "                context_list = [self.contexts[idx]]\n",
    "        else:\n",
    "            context_list = self.contexts[idx]\n",
    "\n",
    "        # Join all context elements\n",
    "        context = \" \".join([str(c) for c in context_list])\n",
    "\n",
    "        # Combine context and comment\n",
    "        combined_text = f\"Context: {context} Comment: {comment}\"\n",
    "\n",
    "        # Create graph data with enhanced features\n",
    "        graph_data, tokens = self.create_graph_from_text(combined_text)\n",
    "\n",
    "        # Encode with truncation and padding for transformer\n",
    "        encoding = self.tokenizer(\n",
    "            combined_text,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].flatten(),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].flatten(),\n",
    "            \"graph_data\": graph_data,\n",
    "            \"tokens\": tokens,\n",
    "            \"label\": torch.tensor(self.labels[idx], dtype=torch.float),\n",
    "        }\n",
    "\n",
    "class GCNLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(GCNLayer, self).__init__()\n",
    "        self.gc = GCNConv(in_features, out_features)\n",
    "        self.bn = nn.BatchNorm1d(out_features)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.gc(x, edge_index)\n",
    "        if x.size(0) > 1:  # BatchNorm needs more than 1 element\n",
    "            x = self.bn(x)\n",
    "        x = F.relu(x)\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers=1, bidirectional=True):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_dim,\n",
    "            hidden_dim,\n",
    "            num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=bidirectional,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch, seq_len, input_dim)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        # Get the output from the last non-padded element\n",
    "        last_output = lstm_out[:, -1, :]\n",
    "        return last_output\n",
    "\n",
    "\n",
    "class SarcasmGCNLSTMDetector(nn.Module):\n",
    "    def __init__(\n",
    "        self, pretrained_model=\"roberta-base\", gcn_hidden_dim=64, dropout_rate=0.3\n",
    "    ):\n",
    "        super(SarcasmGCNLSTMDetector, self).__init__()\n",
    "        self.roberta = RobertaModel.from_pretrained(pretrained_model)\n",
    "        self.hidden_dim = self.roberta.config.hidden_size\n",
    "\n",
    "        # Feature dimensions\n",
    "        feature_dim = EMBEDDING_DIM + 5  # GloVe + Sentiment\n",
    "\n",
    "        # 4-layer GCN as per the paper\n",
    "        self.gcn1 = GCNLayer(feature_dim, gcn_hidden_dim)\n",
    "        self.gcn2 = GCNLayer(gcn_hidden_dim, gcn_hidden_dim * 2)\n",
    "        self.gcn3 = GCNLayer(gcn_hidden_dim * 2, gcn_hidden_dim * 2)\n",
    "        self.gcn4 = GCNLayer(gcn_hidden_dim * 2, gcn_hidden_dim)\n",
    "\n",
    "        # LSTM for sequential processing\n",
    "        self.lstm = LSTM(gcn_hidden_dim, gcn_hidden_dim // 2, bidirectional=True)\n",
    "\n",
    "        # Attention mechanism for combining RoBERTa and GCN-LSTM outputs\n",
    "        self.attention = nn.Linear(self.hidden_dim + gcn_hidden_dim, 1)\n",
    "\n",
    "        # Final classification layers\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc1 = nn.Linear(self.hidden_dim + gcn_hidden_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, graph_x, graph_edge_index):\n",
    "        # Process text with RoBERTa\n",
    "        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        roberta_embedding = outputs.pooler_output  # [CLS] token embedding\n",
    "\n",
    "        # Process graph with multi-layer GCN\n",
    "        x1 = self.gcn1(graph_x, graph_edge_index)\n",
    "        x2 = self.gcn2(x1, graph_edge_index)\n",
    "        x3 = self.gcn3(x2, graph_edge_index)\n",
    "        x4 = self.gcn4(x3, graph_edge_index)\n",
    "\n",
    "        # Prepare for LSTM - reshape if there's a batch\n",
    "        batch_size = roberta_embedding.shape[0]\n",
    "        if batch_size > 1:\n",
    "            # For simplicity, we'll just take the mean of the node embeddings for batched graphs\n",
    "            gcn_embedding = torch.mean(x4, dim=0).unsqueeze(0)\n",
    "            gcn_embedding = gcn_embedding.expand(batch_size, -1)\n",
    "        else:\n",
    "            # Use LSTM for sequential processing (for single example)\n",
    "            # Reshape for LSTM: [num_nodes, features] -> [1, num_nodes, features]\n",
    "            lstm_input = x4.unsqueeze(0)\n",
    "            gcn_embedding = self.lstm(lstm_input)\n",
    "\n",
    "        # Concatenate RoBERTa and GCN-LSTM embeddings\n",
    "        combined = torch.cat((roberta_embedding, gcn_embedding), dim=1)\n",
    "\n",
    "        # Apply attention\n",
    "        attention_weights = torch.sigmoid(self.attention(combined))\n",
    "        weighted_embedding = combined * attention_weights\n",
    "\n",
    "        # Final classification\n",
    "        x = self.dropout(weighted_embedding)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Output logits (not sigmoid)\n",
    "        return self.fc2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(batch):\n",
    "    \"\"\"Custom collate function for handling graph data\"\"\"\n",
    "    # Extract elements from batch\n",
    "    input_ids = torch.stack([item[\"input_ids\"] for item in batch])\n",
    "    attention_mask = torch.stack([item[\"attention_mask\"] for item in batch])\n",
    "    labels = torch.stack([item[\"label\"] for item in batch])\n",
    "    tokens_list = [item[\"tokens\"] for item in batch]\n",
    "\n",
    "    # For graph data, we create a simple representation with batch size of 1\n",
    "    # In a production system, you would use proper batching from PyG\n",
    "    graph_xs = [item[\"graph_data\"].x for item in batch]\n",
    "    graph_edge_indices = [item[\"graph_data\"].edge_index for item in batch]\n",
    "\n",
    "    # Use the first graph for simplicity (or you could merge graphs with proper shifts)\n",
    "    feature_dim = EMBEDDING_DIM + 5  # GloVe + Sentiment\n",
    "    if len(graph_xs) > 0 and graph_xs[0] is not None and graph_xs[0].numel() > 0:\n",
    "        graph_x = graph_xs[0]\n",
    "        graph_edge_index = graph_edge_indices[0]\n",
    "    else:\n",
    "        # Fallback for empty graphs\n",
    "        graph_x = torch.zeros((1, feature_dim), dtype=torch.float)\n",
    "        graph_edge_index = torch.zeros((2, 0), dtype=torch.long)\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"graph_x\": graph_x,\n",
    "        \"graph_edge_index\": graph_edge_index,\n",
    "        \"tokens\": tokens_list,\n",
    "        \"label\": labels,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from ../sarcasm_gcn_lstm_detector_best.pt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n",
      "Model prediction: Sarcastic (Confidence: 0.9583)\n",
      "Full GCN node analysis report saved to sarcasm_gcn_analysis_report.md\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'prediction': 'Sarcastic',\n",
       " 'confidence': 0.9583242535591125,\n",
       " 'tokens': ['congratulations',\n",
       "  'on',\n",
       "  'stating',\n",
       "  'the',\n",
       "  'obvious',\n",
       "  '.',\n",
       "  'i',\n",
       "  'am',\n",
       "  'sure',\n",
       "  'glaciers',\n",
       "  'will',\n",
       "  'start',\n",
       "  'moving',\n",
       "  'any',\n",
       "  'minute',\n",
       "  'now'],\n",
       " 'node_analysis': [{'node_idx': 5,\n",
       "   'token': 'congratulations',\n",
       "   'display_token': 'congratulations',\n",
       "   'final_importance': 10.397899627685547,\n",
       "   'layer_evolution': [4.880075454711914,\n",
       "    10.821146965026855,\n",
       "    12.076385498046875,\n",
       "    10.397899627685547],\n",
       "   'growth_rate': 2.1306841373443604},\n",
       "  {'node_idx': 6,\n",
       "   'token': 'on',\n",
       "   'display_token': 'on',\n",
       "   'final_importance': 7.988095760345459,\n",
       "   'layer_evolution': [4.9547834396362305,\n",
       "    8.539734840393066,\n",
       "    9.949209213256836,\n",
       "    7.988095760345459],\n",
       "   'growth_rate': 1.6121987104415894},\n",
       "  {'node_idx': 13,\n",
       "   'token': 'sure',\n",
       "   'display_token': 'sure',\n",
       "   'final_importance': 7.758020401000977,\n",
       "   'layer_evolution': [5.312359809875488,\n",
       "    8.459394454956055,\n",
       "    11.001701354980469,\n",
       "    7.758020401000977],\n",
       "   'growth_rate': 1.460371732711792},\n",
       "  {'node_idx': 17,\n",
       "   'token': 'moving',\n",
       "   'display_token': 'moving',\n",
       "   'final_importance': 7.634946823120117,\n",
       "   'layer_evolution': [6.0535712242126465,\n",
       "    7.7282586097717285,\n",
       "    9.53664493560791,\n",
       "    7.634946823120117],\n",
       "   'growth_rate': 1.261230230331421},\n",
       "  {'node_idx': 7,\n",
       "   'token': 'stating',\n",
       "   'display_token': 'stating',\n",
       "   'final_importance': 7.6332573890686035,\n",
       "   'layer_evolution': [4.95396614074707,\n",
       "    7.4506120681762695,\n",
       "    9.469017028808594,\n",
       "    7.6332573890686035],\n",
       "   'growth_rate': 1.5408376455307007},\n",
       "  {'node_idx': 8,\n",
       "   'token': 'the',\n",
       "   'display_token': 'the',\n",
       "   'final_importance': 7.593491554260254,\n",
       "   'layer_evolution': [4.079005718231201,\n",
       "    7.081212520599365,\n",
       "    9.305829048156738,\n",
       "    7.593491554260254],\n",
       "   'growth_rate': 1.8616034984588623},\n",
       "  {'node_idx': 12,\n",
       "   'token': 'am',\n",
       "   'display_token': 'am',\n",
       "   'final_importance': 7.306545257568359,\n",
       "   'layer_evolution': [4.5355224609375,\n",
       "    9.10986614227295,\n",
       "    9.943732261657715,\n",
       "    7.306545257568359],\n",
       "   'growth_rate': 1.6109600067138672},\n",
       "  {'node_idx': 16,\n",
       "   'token': 'start',\n",
       "   'display_token': 'start',\n",
       "   'final_importance': 7.2068071365356445,\n",
       "   'layer_evolution': [5.90180778503418,\n",
       "    7.910315036773682,\n",
       "    9.674444198608398,\n",
       "    7.2068071365356445],\n",
       "   'growth_rate': 1.2211185693740845},\n",
       "  {'node_idx': 18,\n",
       "   'token': 'any',\n",
       "   'display_token': 'any',\n",
       "   'final_importance': 7.180318355560303,\n",
       "   'layer_evolution': [5.76493501663208,\n",
       "    7.523026943206787,\n",
       "    8.861504554748535,\n",
       "    7.180318355560303],\n",
       "   'growth_rate': 1.2455159425735474},\n",
       "  {'node_idx': 9,\n",
       "   'token': 'obvious',\n",
       "   'display_token': 'obvious',\n",
       "   'final_importance': 7.146612644195557,\n",
       "   'layer_evolution': [3.6295673847198486,\n",
       "    6.672517776489258,\n",
       "    9.180158615112305,\n",
       "    7.146612644195557],\n",
       "   'growth_rate': 1.9689984321594238},\n",
       "  {'node_idx': 11,\n",
       "   'token': 'i',\n",
       "   'display_token': 'i',\n",
       "   'final_importance': 7.117363929748535,\n",
       "   'layer_evolution': [5.447964668273926,\n",
       "    8.172233581542969,\n",
       "    9.635356903076172,\n",
       "    7.117363929748535],\n",
       "   'growth_rate': 1.3064262866973877},\n",
       "  {'node_idx': 10,\n",
       "   'token': '.',\n",
       "   'display_token': '.',\n",
       "   'final_importance': 7.101311683654785,\n",
       "   'layer_evolution': [5.330958843231201,\n",
       "    6.5875139236450195,\n",
       "    9.45545768737793,\n",
       "    7.101311683654785],\n",
       "   'growth_rate': 1.3320890665054321},\n",
       "  {'node_idx': 14,\n",
       "   'token': 'glaciers',\n",
       "   'display_token': 'glaciers',\n",
       "   'final_importance': 6.993124008178711,\n",
       "   'layer_evolution': [4.719659805297852,\n",
       "    7.820446014404297,\n",
       "    10.169108390808105,\n",
       "    6.993124008178711],\n",
       "   'growth_rate': 1.4817008972167969},\n",
       "  {'node_idx': 19,\n",
       "   'token': 'minute',\n",
       "   'display_token': 'minute',\n",
       "   'final_importance': 6.81332540512085,\n",
       "   'layer_evolution': [5.335178852081299,\n",
       "    7.615298271179199,\n",
       "    8.236139297485352,\n",
       "    6.81332540512085],\n",
       "   'growth_rate': 1.2770565748214722},\n",
       "  {'node_idx': 20,\n",
       "   'token': 'now',\n",
       "   'display_token': 'now',\n",
       "   'final_importance': 6.813324928283691,\n",
       "   'layer_evolution': [5.335178852081299,\n",
       "    7.615298748016357,\n",
       "    8.236139297485352,\n",
       "    6.813324928283691],\n",
       "   'growth_rate': 1.2770565748214722},\n",
       "  {'node_idx': 15,\n",
       "   'token': 'will',\n",
       "   'display_token': 'will',\n",
       "   'final_importance': 6.6040449142456055,\n",
       "   'layer_evolution': [5.329041004180908,\n",
       "    7.5681867599487305,\n",
       "    9.330375671386719,\n",
       "    6.6040449142456055],\n",
       "   'growth_rate': 1.2392557859420776}],\n",
       " 'node_importance_by_layer': [array([8.361853 , 8.97903  , 8.283207 , 7.9168453, 8.397804 , 4.8800755,\n",
       "         4.9547834, 4.953966 , 4.0790057, 3.6295674, 5.330959 , 5.4479647,\n",
       "         4.5355225, 5.31236  , 4.71966  , 5.329041 , 5.901808 , 6.053571 ,\n",
       "         5.764935 , 5.335179 , 5.335179 ], dtype=float32),\n",
       "  array([12.784894 , 13.617905 , 15.248496 , 14.518119 , 10.518857 ,\n",
       "         10.821147 ,  8.539735 ,  7.450612 ,  7.0812125,  6.672518 ,\n",
       "          6.587514 ,  8.172234 ,  9.109866 ,  8.459394 ,  7.820446 ,\n",
       "          7.5681868,  7.910315 ,  7.7282586,  7.523027 ,  7.6152983,\n",
       "          7.6152987], dtype=float32),\n",
       "  array([15.516679 , 15.170387 , 16.24222  , 16.717867 , 13.03342  ,\n",
       "         12.0763855,  9.949209 ,  9.469017 ,  9.305829 ,  9.180159 ,\n",
       "          9.455458 ,  9.635357 ,  9.943732 , 11.001701 , 10.169108 ,\n",
       "          9.330376 ,  9.674444 ,  9.536645 ,  8.861505 ,  8.236139 ,\n",
       "          8.236139 ], dtype=float32),\n",
       "  array([13.209763 , 12.955048 , 13.900982 , 14.232187 , 10.507092 ,\n",
       "         10.3979   ,  7.9880958,  7.6332574,  7.5934916,  7.1466126,\n",
       "          7.1013117,  7.117364 ,  7.3065453,  7.7580204,  6.993124 ,\n",
       "          6.604045 ,  7.206807 ,  7.634947 ,  7.1803184,  6.8133254,\n",
       "          6.813325 ], dtype=float32)],\n",
       " 'graph': <networkx.classes.graph.Graph at 0x7f1582cc0830>}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_PATH = \"../sarcasm_gcn_lstm_detector_best.pt\"\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def load_model_and_tokenizer():\n",
    "    \"\"\"Load the sarcasm detection model and tokenizer\"\"\"\n",
    "    print(f\"Loading model from {MODEL_PATH}...\")\n",
    "    \n",
    "    # Initialize tokenizer\n",
    "    tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "    \n",
    "    # Initialize model\n",
    "    model = SarcasmGCNLSTMDetector().to(DEVICE)\n",
    "    \n",
    "    # Load trained weights\n",
    "    if os.path.exists(MODEL_PATH):\n",
    "        model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
    "        print(\"Model loaded successfully!\")\n",
    "    else:\n",
    "        print(f\"Warning: Model file not found at {MODEL_PATH}\")\n",
    "    \n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "model, tokenizer = load_model_and_tokenizer()\n",
    "# Example usage\n",
    "comment = \"Congratulations on stating the obvious. I am sure glaciers will start moving any minute now\"\n",
    "\n",
    "visualize_gcn_node_influence(model,tokenizer,comment=comment,save_path='sarcasm_gcn_analysis')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
