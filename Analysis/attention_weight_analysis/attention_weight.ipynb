{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1eb1ae60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "import networkx as nx\n",
    "from matplotlib.cm import ScalarMappable\n",
    "from matplotlib.colors import Normalize\n",
    "from PIL import Image\n",
    "from matplotlib.patches import Rectangle\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "def visualize_attention_weights(model, tokenizer, comment, context='', model_path=None, \n",
    "                               device='cuda', layer_indices=None, head_indices=None,\n",
    "                               save_path='attention_visualization', attention_threshold=0.03):\n",
    "    \"\"\"\n",
    "    Visualize attention weights from the RoBERTa component of the sarcasm detection model\n",
    "    \n",
    "    Args:\n",
    "        model: The sarcasm detection model (SarcasmGCNLSTMDetector instance or None if model_path provided)\n",
    "        tokenizer: RoBERTa tokenizer\n",
    "        comment: The comment text to analyze\n",
    "        context: The context for the comment (optional)\n",
    "        model_path: Path to load model from (optional)\n",
    "        device: Device to run model on ('cuda' or 'cpu')\n",
    "        layer_indices: List of layer indices to visualize (if None, use last layer)\n",
    "        head_indices: List of attention head indices to visualize (if None, use all heads)\n",
    "        save_path: Base path to save visualization files\n",
    "        attention_threshold: Threshold for considering attention connections significant\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with attention visualization data\n",
    "    \"\"\"\n",
    "    # Create directory for outputs\n",
    "    os.makedirs(os.path.dirname(save_path) if os.path.dirname(save_path) else '.', exist_ok=True)\n",
    "    \n",
    "    # Load model if path provided\n",
    "    if model_path and model is None:\n",
    "        model = SarcasmGCNLSTMDetector().to(device)\n",
    "        model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    # Format input\n",
    "    if isinstance(context, list):\n",
    "        context = \" \".join([str(c) for c in context if c])\n",
    "    \n",
    "    if context.strip():\n",
    "        combined_text = f\"Context: {context} Comment: {comment}\"\n",
    "    else:\n",
    "        combined_text = comment\n",
    "    \n",
    "    # Tokenize input\n",
    "    encoding = tokenizer(\n",
    "        combined_text,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=128,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "    \n",
    "    # Get number of non-padding tokens\n",
    "    n_tokens = attention_mask.sum().item()\n",
    "    \n",
    "    # Run model with attention output\n",
    "    with torch.no_grad():\n",
    "        # Extract RoBERTa model (assumes first component of hybrid model is RoBERTa)\n",
    "        roberta_model = model.roberta\n",
    "        \n",
    "        # Run RoBERTa with attention outputs\n",
    "        outputs = roberta_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            output_attentions=True\n",
    "        )\n",
    "        \n",
    "        # Get all attention layers\n",
    "        attentions = outputs.attentions\n",
    "        \n",
    "        # Also get the full model prediction\n",
    "        try:\n",
    "            # Create empty placeholder for GCN\n",
    "            graph_x = torch.zeros((1, 305), dtype=torch.float).to(device)  # GloVe + Sentiment dimensions\n",
    "            graph_edge_index = torch.zeros((2, 0), dtype=torch.long).to(device)\n",
    "            \n",
    "            logits = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                graph_x=graph_x,\n",
    "                graph_edge_index=graph_edge_index\n",
    "            )\n",
    "            \n",
    "            prediction_prob = torch.sigmoid(logits).item()\n",
    "            prediction = \"Sarcastic\" if prediction_prob > 0.5 else \"Not Sarcastic\"\n",
    "            confidence = prediction_prob if prediction_prob > 0.5 else 1 - prediction_prob\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not get full model prediction due to error: {str(e)}\")\n",
    "            prediction = \"Unknown\"\n",
    "            confidence = 0.0\n",
    "    \n",
    "    # Convert token ids to tokens\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "    tokens = tokens[:n_tokens]  # Remove padding tokens\n",
    "    \n",
    "    # MODIFICATION: Remove special tokens <s> and </s> and clean token representations\n",
    "    # First, find indices of special tokens\n",
    "    special_tokens_indices = []\n",
    "    for i, token in enumerate(tokens):\n",
    "        if token in ['<s>', '</s>']:\n",
    "            special_tokens_indices.append(i)\n",
    "    \n",
    "    # Filter out special tokens and adjust attention matrices\n",
    "    if special_tokens_indices:\n",
    "        # Create a mask for non-special tokens\n",
    "        valid_indices = [i for i in range(len(tokens)) if i not in special_tokens_indices]\n",
    "        \n",
    "        # Update token list - remove special tokens\n",
    "        tokens = [tokens[i] for i in valid_indices]\n",
    "        \n",
    "        # Clean tokens - remove \"Ġ\" prefix from tokens\n",
    "        tokens = [token.replace('Ġ', '') for token in tokens]\n",
    "        \n",
    "        # Adjust attention matrices for all layers\n",
    "        modified_attentions = []\n",
    "        for layer_attn in attentions:\n",
    "            # Extract and keep only rows and columns for non-special tokens\n",
    "            layer_valid = layer_attn[0, :, valid_indices, :][:, :, valid_indices]\n",
    "            modified_attentions.append(layer_valid.unsqueeze(0))\n",
    "        \n",
    "        # Replace original attentions with modified ones\n",
    "        attentions = tuple(modified_attentions)\n",
    "        \n",
    "        # Update token count\n",
    "        n_tokens = len(tokens)\n",
    "    else:\n",
    "        # Just clean tokens if no special tokens found\n",
    "        tokens = [token.replace('Ġ', '') for token in tokens]\n",
    "    \n",
    "    # Process layer indices\n",
    "    num_layers = len(attentions)\n",
    "    if layer_indices is None:\n",
    "        layer_indices = [num_layers - 1]  # Default to last layer\n",
    "    else:\n",
    "        # Ensure layer indices are valid\n",
    "        layer_indices = [i for i in layer_indices if i < num_layers]\n",
    "        if not layer_indices:\n",
    "            layer_indices = [num_layers - 1]  # Default to last layer if none are valid\n",
    "    \n",
    "    # Process head indices\n",
    "    num_heads = attentions[0].size(1)\n",
    "    if head_indices is None:\n",
    "        head_indices = list(range(num_heads))  # Default to all heads\n",
    "    else:\n",
    "        # Ensure head indices are valid\n",
    "        head_indices = [i for i in head_indices if i < num_heads]\n",
    "        if not head_indices:\n",
    "            head_indices = list(range(num_heads))  # Default to all heads if none are valid\n",
    "    \n",
    "    print(f\"Model prediction: {prediction} (Confidence: {confidence:.4f})\")\n",
    "    print(f\"Number of layers: {num_layers}, Number of heads per layer: {num_heads}\")\n",
    "    print(f\"Analyzing {len(layer_indices)} layers and {len(head_indices)} heads per layer\")\n",
    "    \n",
    "    # Create visualizations\n",
    "    attention_data = {}\n",
    "    \n",
    "    # 1. Heatmaps for individual attention heads\n",
    "    for layer_idx in layer_indices:\n",
    "        for head_idx in head_indices:\n",
    "            # Extract attention matrix for this head\n",
    "            attention_matrix = attentions[layer_idx][0, head_idx, :n_tokens, :n_tokens].cpu().numpy()\n",
    "            \n",
    "            # Create heatmap\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            ax = plt.subplot()\n",
    "            \n",
    "            # Plot heatmap\n",
    "            sns.heatmap(attention_matrix, cmap='viridis', xticklabels=tokens, yticklabels=tokens)\n",
    "            \n",
    "            # Format plot\n",
    "            plt.title(f\"Attention Weights - Layer {layer_idx+1}, Head {head_idx+1}\")\n",
    "            plt.xlabel(\"Token (attention to)\")\n",
    "            plt.ylabel(\"Token (attention from)\")\n",
    "            \n",
    "            # Rotate x-axis labels for readability\n",
    "            plt.xticks(rotation=45, ha='right')\n",
    "            plt.yticks(rotation=0)\n",
    "            \n",
    "            # Save figure\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f\"{save_path}_layer{layer_idx+1}_head{head_idx+1}.png\", dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            \n",
    "            # Store data\n",
    "            attention_data[f\"layer{layer_idx+1}_head{head_idx+1}\"] = attention_matrix\n",
    "    \n",
    "    # 2. Aggregated attention map (average across selected heads)\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    # Initialize aggregated matrix\n",
    "    aggregated_attention = np.zeros((n_tokens, n_tokens))\n",
    "    count = 0\n",
    "    \n",
    "    # Aggregate attention weights\n",
    "    for layer_idx in layer_indices:\n",
    "        for head_idx in head_indices:\n",
    "            aggregated_attention += attentions[layer_idx][0, head_idx, :n_tokens, :n_tokens].cpu().numpy()\n",
    "            count += 1\n",
    "    \n",
    "    aggregated_attention /= count\n",
    "    \n",
    "    # Plot aggregated heatmap\n",
    "    sns.heatmap(aggregated_attention, cmap='viridis', xticklabels=tokens, yticklabels=tokens)\n",
    "    \n",
    "    # Format plot\n",
    "    plt.title(f\"Aggregated Attention Weights (Average across {count} heads)\")\n",
    "    plt.xlabel(\"Token (attention to)\")\n",
    "    plt.ylabel(\"Token (attention from)\")\n",
    "    \n",
    "    # Rotate x-axis labels for readability\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    \n",
    "    # Save figure\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{save_path}_aggregated.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # Store aggregated data\n",
    "    attention_data[\"aggregated\"] = aggregated_attention\n",
    "    \n",
    "    # 3. Attention flow graph visualization for the most important layer and head\n",
    "    # Find the head with highest attention entropy (most informative)\n",
    "    max_entropy = -float('inf')\n",
    "    best_layer = 0\n",
    "    best_head = 0\n",
    "    \n",
    "    for layer_idx in layer_indices:\n",
    "        for head_idx in head_indices:\n",
    "            attention_matrix = attentions[layer_idx][0, head_idx, :n_tokens, :n_tokens].cpu().numpy()\n",
    "            \n",
    "            # Calculate entropy for this attention distribution\n",
    "            # Higher entropy means more distributed attention (less focused)\n",
    "            entropy = 0\n",
    "            for i in range(n_tokens):\n",
    "                row = attention_matrix[i]\n",
    "                row = row / (row.sum() + 1e-10)  # Normalize\n",
    "                row_entropy = -np.sum(row * np.log(row + 1e-10))\n",
    "                entropy += row_entropy\n",
    "            \n",
    "            if entropy > max_entropy:\n",
    "                max_entropy = entropy\n",
    "                best_layer = layer_idx\n",
    "                best_head = head_idx\n",
    "    \n",
    "    # Create attention flow graph for most informative head\n",
    "    attention_matrix = attentions[best_layer][0, best_head, :n_tokens, :n_tokens].cpu().numpy()\n",
    "    \n",
    "    # Create a graph\n",
    "    G = nx.DiGraph()\n",
    "    \n",
    "    # Add nodes\n",
    "    for i, token in enumerate(tokens):\n",
    "        G.add_node(i, token=token)\n",
    "    \n",
    "    # Add weighted edges (only add edges with significant attention weight)\n",
    "    edge_weights = []\n",
    "    for i in range(n_tokens):\n",
    "        for j in range(n_tokens):\n",
    "            if attention_matrix[i, j] > attention_threshold:\n",
    "                G.add_edge(i, j, weight=attention_matrix[i, j])\n",
    "                edge_weights.append(attention_matrix[i, j])\n",
    "    \n",
    "    # Visualize the graph - handle empty edge_weights case\n",
    "    plt.figure(figsize=(14, 12))\n",
    "    \n",
    "    if edge_weights:  # Only proceed if we have edges with weights above threshold\n",
    "        # Create layout (try to make it more readable)\n",
    "        pos = nx.spring_layout(G, k=0.5, iterations=50)\n",
    "        \n",
    "        # Draw the graph\n",
    "        nx.draw_networkx_nodes(G, pos, node_size=700, node_color='skyblue', alpha=0.8)\n",
    "        nx.draw_networkx_labels(G, pos, labels={i: data['token'] for i, data in G.nodes(data=True)})\n",
    "        \n",
    "        # Only draw edges if we have them\n",
    "        if G.edges():\n",
    "            # Get edge weights for width and color\n",
    "            edge_widths = [G[u][v]['weight'] * 10 for u, v in G.edges()]\n",
    "            \n",
    "            nx.draw_networkx_edges(G, pos, width=edge_widths, alpha=0.7, \n",
    "                                edge_color=edge_weights, edge_cmap=plt.cm.Reds,\n",
    "                                connectionstyle='arc3,rad=0.1')  # Curved edges for better visibility\n",
    "            \n",
    "            # Add a colorbar\n",
    "            sm = ScalarMappable(cmap=plt.cm.Reds, norm=Normalize(vmin=min(edge_weights), vmax=max(edge_weights)))\n",
    "            sm.set_array([])\n",
    "            cax = plt.axes([0.92, 0.1, 0.02, 0.8])  # Position for colorbar [left, bottom, width, height]\n",
    "            plt.colorbar(sm, cax=cax, label='Attention Weight')\n",
    "        else:\n",
    "            # No edges, just add a text note\n",
    "            plt.figtext(0.5, 0.5, \"No strong attention connections found\", \n",
    "                      ha='center', va='center', fontsize=12,\n",
    "                      bbox=dict(boxstyle=\"round\", fc=\"white\", ec=\"gray\", alpha=0.8))\n",
    "    else:\n",
    "        # No edges with weights above threshold, display a message\n",
    "        plt.text(0.5, 0.5, f\"No significant attention connections found above threshold {attention_threshold}\", \n",
    "                ha='center', va='center', fontsize=12, transform=plt.gca().transAxes,\n",
    "                bbox=dict(boxstyle=\"round\", fc=\"white\", ec=\"gray\", alpha=0.8))\n",
    "    \n",
    "    plt.title(f\"Attention Flow Graph (Layer {best_layer+1}, Head {best_head+1})\")\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{save_path}_flow_graph.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # 4. Layer comparison visualization\n",
    "    if len(layer_indices) > 1:\n",
    "        # Analyze how attention patterns change across layers\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        \n",
    "        # Calculate average attention per layer\n",
    "        layer_avg_attention = []\n",
    "        for layer_idx in layer_indices:\n",
    "            # Average across selected heads\n",
    "            layer_attention = np.zeros((n_tokens, n_tokens))\n",
    "            for head_idx in head_indices:\n",
    "                layer_attention += attentions[layer_idx][0, head_idx, :n_tokens, :n_tokens].cpu().numpy()\n",
    "            layer_attention /= len(head_indices)\n",
    "            layer_avg_attention.append(layer_attention)\n",
    "        \n",
    "        # Number of layers to visualize\n",
    "        n_layers = len(layer_indices)\n",
    "        \n",
    "        # Calculate grid size\n",
    "        n_cols = min(3, n_layers)\n",
    "        n_rows = (n_layers + n_cols - 1) // n_cols\n",
    "        \n",
    "        # Plot each layer's attention\n",
    "        for i, layer_idx in enumerate(layer_indices):\n",
    "            plt.subplot(n_rows, n_cols, i+1)\n",
    "            sns.heatmap(layer_avg_attention[i], cmap='viridis', xticklabels=False, yticklabels=False)\n",
    "            plt.title(f\"Layer {layer_idx+1}\")\n",
    "        \n",
    "        plt.suptitle(\"Attention Patterns Across Layers (Averaged Across Heads)\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{save_path}_layer_comparison.png\", dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    \n",
    "    # 5. Attention distribution analysis\n",
    "    # Analyze how attention is distributed for each token\n",
    "    token_attention_stats = []\n",
    "    \n",
    "    for token_idx in range(n_tokens):\n",
    "        token = tokens[token_idx]\n",
    "        \n",
    "        # Get attention statistics for this token across layers and heads\n",
    "        token_stats = {\n",
    "            'token': token,\n",
    "            'token_idx': token_idx,\n",
    "            'self_attention': []  # How much attention this token pays to itself\n",
    "        }\n",
    "        \n",
    "        for layer_idx in layer_indices:\n",
    "            for head_idx in head_indices:\n",
    "                # Self-attention (attention to self)\n",
    "                self_attn = attentions[layer_idx][0, head_idx, token_idx, token_idx].item()\n",
    "                token_stats['self_attention'].append(self_attn)\n",
    "        \n",
    "        # Calculate average self-attention\n",
    "        token_stats['avg_self_attention'] = np.mean(token_stats['self_attention'])\n",
    "        \n",
    "        # Calculate attention focus (higher means more focused attention on fewer tokens)\n",
    "        attention_focus = []\n",
    "        for layer_idx in layer_indices:\n",
    "            for head_idx in head_indices:\n",
    "                attn_dist = attentions[layer_idx][0, head_idx, token_idx, :n_tokens].cpu().numpy()\n",
    "                # Use Gini coefficient as a measure of attention focus\n",
    "                sorted_attn = np.sort(attn_dist)\n",
    "                n = len(sorted_attn)\n",
    "                index = np.arange(1, n+1)\n",
    "                gini = 1 - 2 * np.sum((n + 1 - index) * sorted_attn) / (n * np.sum(sorted_attn))\n",
    "                attention_focus.append(gini)\n",
    "        \n",
    "        token_stats['avg_attention_focus'] = np.mean(attention_focus)\n",
    "        \n",
    "        token_attention_stats.append(token_stats)\n",
    "    \n",
    "    # Create token attention analysis plot\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # Sort tokens by their position in the text\n",
    "    token_indices = [stat['token_idx'] for stat in token_attention_stats]\n",
    "    tokens_for_plot = [stat['token'] for stat in token_attention_stats]\n",
    "    avg_self_attention = [stat['avg_self_attention'] for stat in token_attention_stats]\n",
    "    avg_attention_focus = [stat['avg_attention_focus'] for stat in token_attention_stats]\n",
    "    \n",
    "    # Plot attention statistics\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.bar(token_indices, avg_self_attention, color='cornflowerblue')\n",
    "    plt.xticks(token_indices, tokens_for_plot, rotation=45, ha='right')\n",
    "    plt.title('Average Self-Attention by Token')\n",
    "    plt.ylabel('Self-Attention Weight')\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.bar(token_indices, avg_attention_focus, color='coral')\n",
    "    plt.xticks(token_indices, tokens_for_plot, rotation=45, ha='right')\n",
    "    plt.title('Attention Focus by Token (higher = more focused)')\n",
    "    plt.ylabel('Attention Focus (Gini)')\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{save_path}_token_analysis.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # 6. Create a summary report\n",
    "    try:\n",
    "        # Generate a markdown report\n",
    "        report = f\"\"\"\n",
    "# Attention Analysis Report for Sarcasm Detection\n",
    "\n",
    "## Overview\n",
    "- **Text**: \"{comment}\"\n",
    "- **Context**: \"{context}\"\n",
    "- **Prediction**: {prediction} (Confidence: {confidence:.4f})\n",
    "- **Analyzed**: {len(layer_indices)} layers, {len(head_indices)} heads per layer\n",
    "\n",
    "## Key Findings\n",
    "\n",
    "### Most Attentive Tokens\n",
    "The following tokens receive the most attention:\n",
    "        \n",
    "| Token | Position | Self-Attention | Attention Focus |\n",
    "|-------|----------|----------------|----------------|\n",
    "\"\"\"\n",
    "        \n",
    "        # Sort tokens by attention received\n",
    "        sorted_tokens = sorted(token_attention_stats, key=lambda x: x['avg_self_attention'], reverse=True)\n",
    "        for i, token_stat in enumerate(sorted_tokens[:5]):  # Top 5 tokens\n",
    "            report += f\"| {token_stat['token']} | {token_stat['token_idx']} | {token_stat['avg_self_attention']:.4f} | {token_stat['avg_attention_focus']:.4f} |\\n\"\n",
    "        \n",
    "        report += \"\"\"\n",
    "### Attention Patterns\n",
    "\"\"\"\n",
    "        \n",
    "        # Find special attention patterns\n",
    "        # 1. Tokens that attend strongly to each other (potential connections)\n",
    "        strong_connections = []\n",
    "        \n",
    "        # Use the aggregated attention matrix to find strong connections\n",
    "        for i in range(n_tokens):\n",
    "            for j in range(n_tokens):\n",
    "                if i != j and aggregated_attention[i, j] > 0.1:  # Threshold for strong connection\n",
    "                    strong_connections.append({\n",
    "                        'from': tokens[i],\n",
    "                        'to': tokens[j],\n",
    "                        'weight': aggregated_attention[i, j]\n",
    "                    })\n",
    "        \n",
    "        # Sort by connection strength\n",
    "        strong_connections = sorted(strong_connections, key=lambda x: x['weight'], reverse=True)\n",
    "        \n",
    "        if strong_connections:\n",
    "            report += \"\"\"\n",
    "#### Strong Token Connections\n",
    "The following token pairs show strong attention connections:\n",
    "\n",
    "| From | To | Attention Weight |\n",
    "|------|----|-----------------:|\n",
    "\"\"\"\n",
    "            \n",
    "            for i, conn in enumerate(strong_connections[:5]):  # Top 5 connections\n",
    "                report += f\"| {conn['from']} | {conn['to']} | {conn['weight']:.4f} |\\n\"\n",
    "        \n",
    "        # Save report\n",
    "        with open(f\"{save_path}_report.md\", 'w') as f:\n",
    "            f.write(report)\n",
    "        \n",
    "        print(f\"Full attention analysis report saved to {save_path}_report.md\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error generating report: {str(e)}\")\n",
    "    \n",
    "    # Return attention data\n",
    "    return {\n",
    "        'prediction': prediction,\n",
    "        'confidence': confidence,\n",
    "        'tokens': tokens,\n",
    "        'attention_data': attention_data,\n",
    "        'aggregated_attention': aggregated_attention,\n",
    "        'token_attention_stats': token_attention_stats,\n",
    "        'best_layer': best_layer,\n",
    "        'best_head': best_head\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2a9dbf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GloVe embeddings...\n",
      "Loaded GloVe embeddings with dimension: 300\n",
      "Loaded spaCy model successfully\n",
      "Loaded SenticNet successfully\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    RobertaTokenizer,\n",
    "    RobertaModel,\n",
    "    # AdamW,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "from torch.optim import AdamW\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "import ast\n",
    "import os\n",
    "import gc\n",
    "import networkx as nx\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import from_networkx\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.utils import resample\n",
    "import spacy\n",
    "from senticnet.senticnet import SenticNet\n",
    "import gensim.downloader as gensim_downloader\n",
    "\n",
    "print(\"Loading GloVe embeddings...\")\n",
    "try:\n",
    "    glove_embeddings = gensim_downloader.load(\"glove-wiki-gigaword-300\")\n",
    "    EMBEDDING_DIM = 300\n",
    "    print(f\"Loaded GloVe embeddings with dimension: {EMBEDDING_DIM}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading GloVe embeddings: {str(e)}\")\n",
    "    print(\"Using random embeddings instead\")\n",
    "    glove_embeddings = None\n",
    "    EMBEDDING_DIM = 300\n",
    "\n",
    "# Load spaCy model\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    print(\"Loaded spaCy model successfully\")\n",
    "except:\n",
    "    print(\"Downloading spaCy model...\")\n",
    "    import subprocess\n",
    "\n",
    "    subprocess.run([\"python\", \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"])\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Initialize SenticNet\n",
    "try:\n",
    "    sn = SenticNet()\n",
    "    print(\"Loaded SenticNet successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading SenticNet: {str(e)}\")\n",
    "    sn = None\n",
    "\n",
    "class SarcasmGraphDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length=128):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.comments = df[\"comment\"].values\n",
    "        self.contexts = df[\"context\"].values\n",
    "        self.labels = df[\"label\"].values\n",
    "        self.max_length = max_length\n",
    "        self.window_size = 2  # Window size for graph construction\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.comments)\n",
    "\n",
    "    def get_embedding(self, word):\n",
    "        \"\"\"Get the GloVe embedding for a word\"\"\"\n",
    "        word = word.lower()\n",
    "        if glove_embeddings and word in glove_embeddings:\n",
    "            return torch.tensor(glove_embeddings[word], dtype=torch.float)\n",
    "        else:\n",
    "            # Use random embedding if word not found\n",
    "            return torch.randn(EMBEDDING_DIM, dtype=torch.float)\n",
    "\n",
    "    def get_sentiment_features(self, word):\n",
    "        \"\"\"Extract sentiment features using SenticNet\"\"\"\n",
    "        try:\n",
    "            if sn is not None:\n",
    "                concept_info = sn.concept(word)\n",
    "                # Extract polarity value (float between -1 and 1)\n",
    "                polarity = float(concept_info[\"polarity_value\"])\n",
    "                # Create a 5-dimensional feature: [polarity, is_positive, is_negative, is_neutral, intensity]\n",
    "                is_positive = 1.0 if polarity > 0.1 else 0.0\n",
    "                is_negative = 1.0 if polarity < -0.1 else 0.0\n",
    "                is_neutral = 1.0 if abs(polarity) <= 0.1 else 0.0\n",
    "                intensity = abs(polarity)\n",
    "                return torch.tensor(\n",
    "                    [polarity, is_positive, is_negative, is_neutral, intensity],\n",
    "                    dtype=torch.float,\n",
    "                )\n",
    "            else:\n",
    "                return torch.zeros(5, dtype=torch.float)\n",
    "        except:\n",
    "            # Word not found in SenticNet\n",
    "            return torch.zeros(5, dtype=torch.float)\n",
    "\n",
    "    def create_graph_from_text(self, text):\n",
    "        \"\"\"Create a graph representation of text for GCN with enhanced features\"\"\"\n",
    "        # Parse text with spaCy for dependency parsing\n",
    "        doc = nlp(text.lower())\n",
    "\n",
    "        # Create a graph where nodes are tokens\n",
    "        G = nx.Graph()\n",
    "\n",
    "        # Store tokens for later embedding lookup\n",
    "        tokens = [token.text for token in doc]\n",
    "\n",
    "        # Add nodes with positions\n",
    "        for i, token in enumerate(doc):\n",
    "            G.add_node(i, word=token.text, pos=token.pos_)\n",
    "\n",
    "        # Add edges based on window and dependencies\n",
    "        # 1. Window-based edges\n",
    "        for i in range(len(tokens)):\n",
    "            for j in range(i + 1, min(i + self.window_size + 1, len(tokens))):\n",
    "                G.add_edge(i, j, edge_type=0)  # Type 0: window edge\n",
    "\n",
    "        # 2. Dependency-based edges\n",
    "        for token in doc:\n",
    "            if token.i < len(tokens) and token.head.i < len(tokens):\n",
    "                G.add_edge(\n",
    "                    token.i, token.head.i, edge_type=1\n",
    "                )  # Type 1: dependency edge\n",
    "\n",
    "        # Convert to PyTorch Geometric Data object\n",
    "        if len(G.nodes) > 0:\n",
    "            data = from_networkx(G)\n",
    "\n",
    "            # Create feature matrix for nodes [GloVe (25d) + Sentiment (5d)]\n",
    "            feature_dim = EMBEDDING_DIM + 5\n",
    "            features = torch.zeros((len(G.nodes), feature_dim), dtype=torch.float)\n",
    "\n",
    "            for i, token_text in enumerate(tokens):\n",
    "                if i < len(features):\n",
    "                    # GloVe embedding\n",
    "                    glove_feature = self.get_embedding(token_text)\n",
    "                    # Sentiment features\n",
    "                    sentiment_feature = self.get_sentiment_features(token_text)\n",
    "                    # Concatenate features\n",
    "                    if (\n",
    "                        len(glove_feature) == EMBEDDING_DIM\n",
    "                        and len(sentiment_feature) == 5\n",
    "                    ):\n",
    "                        features[i] = torch.cat([glove_feature, sentiment_feature])\n",
    "\n",
    "            data.x = features\n",
    "            return data, tokens\n",
    "        else:\n",
    "            # Return empty graph if there are no nodes\n",
    "            empty_data = Data(\n",
    "                x=torch.zeros((1, feature_dim), dtype=torch.float),\n",
    "                edge_index=torch.zeros((2, 0), dtype=torch.long),\n",
    "            )\n",
    "            return empty_data, []\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        comment = str(self.comments[idx])\n",
    "\n",
    "        # Parse context if it's a string\n",
    "        if isinstance(self.contexts[idx], str):\n",
    "            try:\n",
    "                context_list = ast.literal_eval(self.contexts[idx])\n",
    "            except:\n",
    "                context_list = [self.contexts[idx]]\n",
    "        else:\n",
    "            context_list = self.contexts[idx]\n",
    "\n",
    "        # Join all context elements\n",
    "        context = \" \".join([str(c) for c in context_list])\n",
    "\n",
    "        # Combine context and comment\n",
    "        combined_text = f\"Context: {context} Comment: {comment}\"\n",
    "\n",
    "        # Create graph data with enhanced features\n",
    "        graph_data, tokens = self.create_graph_from_text(combined_text)\n",
    "\n",
    "        # Encode with truncation and padding for transformer\n",
    "        encoding = self.tokenizer(\n",
    "            combined_text,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].flatten(),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].flatten(),\n",
    "            \"graph_data\": graph_data,\n",
    "            \"tokens\": tokens,\n",
    "            \"label\": torch.tensor(self.labels[idx], dtype=torch.float),\n",
    "        }\n",
    "\n",
    "class GCNLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(GCNLayer, self).__init__()\n",
    "        self.gc = GCNConv(in_features, out_features)\n",
    "        self.bn = nn.BatchNorm1d(out_features)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.gc(x, edge_index)\n",
    "        if x.size(0) > 1:  # BatchNorm needs more than 1 element\n",
    "            x = self.bn(x)\n",
    "        x = F.relu(x)\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers=1, bidirectional=True):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_dim,\n",
    "            hidden_dim,\n",
    "            num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=bidirectional,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch, seq_len, input_dim)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        # Get the output from the last non-padded element\n",
    "        last_output = lstm_out[:, -1, :]\n",
    "        return last_output\n",
    "\n",
    "\n",
    "class SarcasmGCNLSTMDetector(nn.Module):\n",
    "    def __init__(\n",
    "        self, pretrained_model=\"roberta-base\", gcn_hidden_dim=64, dropout_rate=0.3\n",
    "    ):\n",
    "        super(SarcasmGCNLSTMDetector, self).__init__()\n",
    "        self.roberta = RobertaModel.from_pretrained(pretrained_model)\n",
    "        self.hidden_dim = self.roberta.config.hidden_size\n",
    "\n",
    "        # Feature dimensions\n",
    "        feature_dim = EMBEDDING_DIM + 5  # GloVe + Sentiment\n",
    "\n",
    "        # 4-layer GCN as per the paper\n",
    "        self.gcn1 = GCNLayer(feature_dim, gcn_hidden_dim)\n",
    "        self.gcn2 = GCNLayer(gcn_hidden_dim, gcn_hidden_dim * 2)\n",
    "        self.gcn3 = GCNLayer(gcn_hidden_dim * 2, gcn_hidden_dim * 2)\n",
    "        self.gcn4 = GCNLayer(gcn_hidden_dim * 2, gcn_hidden_dim)\n",
    "\n",
    "        # LSTM for sequential processing\n",
    "        self.lstm = LSTM(gcn_hidden_dim, gcn_hidden_dim // 2, bidirectional=True)\n",
    "\n",
    "        # Attention mechanism for combining RoBERTa and GCN-LSTM outputs\n",
    "        self.attention = nn.Linear(self.hidden_dim + gcn_hidden_dim, 1)\n",
    "\n",
    "        # Final classification layers\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc1 = nn.Linear(self.hidden_dim + gcn_hidden_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, graph_x, graph_edge_index):\n",
    "        # Process text with RoBERTa\n",
    "        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        roberta_embedding = outputs.pooler_output  # [CLS] token embedding\n",
    "\n",
    "        # Process graph with multi-layer GCN\n",
    "        x1 = self.gcn1(graph_x, graph_edge_index)\n",
    "        x2 = self.gcn2(x1, graph_edge_index)\n",
    "        x3 = self.gcn3(x2, graph_edge_index)\n",
    "        x4 = self.gcn4(x3, graph_edge_index)\n",
    "\n",
    "        # Prepare for LSTM - reshape if there's a batch\n",
    "        batch_size = roberta_embedding.shape[0]\n",
    "        if batch_size > 1:\n",
    "            # For simplicity, we'll just take the mean of the node embeddings for batched graphs\n",
    "            gcn_embedding = torch.mean(x4, dim=0).unsqueeze(0)\n",
    "            gcn_embedding = gcn_embedding.expand(batch_size, -1)\n",
    "        else:\n",
    "            # Use LSTM for sequential processing (for single example)\n",
    "            # Reshape for LSTM: [num_nodes, features] -> [1, num_nodes, features]\n",
    "            lstm_input = x4.unsqueeze(0)\n",
    "            gcn_embedding = self.lstm(lstm_input)\n",
    "\n",
    "        # Concatenate RoBERTa and GCN-LSTM embeddings\n",
    "        combined = torch.cat((roberta_embedding, gcn_embedding), dim=1)\n",
    "\n",
    "        # Apply attention\n",
    "        attention_weights = torch.sigmoid(self.attention(combined))\n",
    "        weighted_embedding = combined * attention_weights\n",
    "\n",
    "        # Final classification\n",
    "        x = self.dropout(weighted_embedding)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Output logits (not sigmoid)\n",
    "        return self.fc2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c805d82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(batch):\n",
    "    \"\"\"Custom collate function for handling graph data\"\"\"\n",
    "    # Extract elements from batch\n",
    "    input_ids = torch.stack([item[\"input_ids\"] for item in batch])\n",
    "    attention_mask = torch.stack([item[\"attention_mask\"] for item in batch])\n",
    "    labels = torch.stack([item[\"label\"] for item in batch])\n",
    "    tokens_list = [item[\"tokens\"] for item in batch]\n",
    "\n",
    "    # For graph data, we create a simple representation with batch size of 1\n",
    "    # In a production system, you would use proper batching from PyG\n",
    "    graph_xs = [item[\"graph_data\"].x for item in batch]\n",
    "    graph_edge_indices = [item[\"graph_data\"].edge_index for item in batch]\n",
    "\n",
    "    # Use the first graph for simplicity (or you could merge graphs with proper shifts)\n",
    "    feature_dim = EMBEDDING_DIM + 5  # GloVe + Sentiment\n",
    "    if len(graph_xs) > 0 and graph_xs[0] is not None and graph_xs[0].numel() > 0:\n",
    "        graph_x = graph_xs[0]\n",
    "        graph_edge_index = graph_edge_indices[0]\n",
    "    else:\n",
    "        # Fallback for empty graphs\n",
    "        graph_x = torch.zeros((1, feature_dim), dtype=torch.float)\n",
    "        graph_edge_index = torch.zeros((2, 0), dtype=torch.long)\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"graph_x\": graph_x,\n",
    "        \"graph_edge_index\": graph_edge_index,\n",
    "        \"tokens\": tokens_list,\n",
    "        \"label\": labels,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09248595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from ../sarcasm_gcn_lstm_detector_best.pt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "RobertaSdpaSelfAttention is used but `torch.nn.functional.scaled_dot_product_attention` does not support non-absolute `position_embedding_type` or `output_attentions=True` or `head_mask`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n",
      "Model prediction: Sarcastic (Confidence: 0.9531)\n",
      "Number of layers: 12, Number of heads per layer: 12\n",
      "Analyzing 3 layers and 12 heads per layer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3629447/2209769790.py:317: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n",
      "  plt.tight_layout()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full attention analysis report saved to ./sarcasm_attention/sarcasm_attention_report.md\n"
     ]
    }
   ],
   "source": [
    "MODEL_PATH = \"../sarcasm_gcn_lstm_detector_best.pt\"\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def load_model_and_tokenizer():\n",
    "    \"\"\"Load the sarcasm detection model and tokenizer\"\"\"\n",
    "    print(f\"Loading model from {MODEL_PATH}...\")\n",
    "    \n",
    "    # Initialize tokenizer\n",
    "    tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "    \n",
    "    # Initialize model\n",
    "    model = SarcasmGCNLSTMDetector().to(DEVICE)\n",
    "    \n",
    "    # Load trained weights\n",
    "    if os.path.exists(MODEL_PATH):\n",
    "        model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
    "        print(\"Model loaded successfully!\")\n",
    "    else:\n",
    "        print(f\"Warning: Model file not found at {MODEL_PATH}\")\n",
    "    \n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "model, tokenizer = load_model_and_tokenizer()\n",
    "# Example usage\n",
    "comment = \"Congratulations on stating the obvious. I am sure glaciers will start moving any minute now\"\n",
    "save_path = \"./sarcasm_attention/sarcasm_attention\"\n",
    "attention_data = visualize_attention_weights(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    comment,\n",
    "    # context=context,\n",
    "    device=DEVICE,\n",
    "    layer_indices=[0, 5, 11],\n",
    "    head_indices=None,\n",
    "    save_path=save_path,\n",
    "    attention_threshold=0.1\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
