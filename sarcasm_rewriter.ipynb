{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, BertTokenizer, BertModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "import random\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Download nltk data if needed\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Custom dataset\n",
    "class SarcasmGANDataset(Dataset):\n",
    "    def __init__(self, csv_path, tokenizer, bert_tokenizer, max_length=128, prompt_style=\"instruction\"):\n",
    "        df = pd.read_csv(csv_path)\n",
    "        self.sarcastic = df['tweet'].astype(str).tolist()\n",
    "        self.rephrase = df['rephrase'].astype(str).tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.bert_tokenizer = bert_tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.prompt_style = prompt_style\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.sarcastic)\n",
    "        \n",
    "    def get_prompt(self, text):\n",
    "        \"\"\"Multi-style prompting for better instruction following\"\"\"\n",
    "        styles = {\n",
    "            \"instruction\": f\"Remove sarcasm from the following text: {text}\",\n",
    "            \"rewrite\": f\"Rewrite without sarcasm: {text}\",\n",
    "            \"transform\": f\"Transform this sarcastic statement into a non-sarcastic one: {text}\",\n",
    "            \"neutral\": f\"Make this statement neutral and straightforward: {text}\"\n",
    "        }\n",
    "        \n",
    "        if self.prompt_style == \"random\":\n",
    "            return styles[list(styles.keys())[np.random.randint(0, len(styles))]]\n",
    "        return styles[self.prompt_style]\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        src = self.sarcastic[idx]\n",
    "        tgt = self.rephrase[idx]\n",
    "        \n",
    "        # Apply prompt template with clear instructions\n",
    "        src_prompted = self.get_prompt(src)\n",
    "        \n",
    "        # Tokenize for generator\n",
    "        src_encoded = self.tokenizer(\n",
    "            src_prompted,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        tgt_encoded = self.tokenizer(\n",
    "            tgt,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Tokenize for discriminator\n",
    "        src_disc = self.bert_tokenizer(\n",
    "            src,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        tgt_disc = self.bert_tokenizer(\n",
    "            tgt,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        input_ids = src_encoded[\"input_ids\"].squeeze()\n",
    "        attention_mask = src_encoded[\"attention_mask\"].squeeze()\n",
    "        labels = tgt_encoded[\"input_ids\"].squeeze()\n",
    "        \n",
    "        disc_src_ids = src_disc[\"input_ids\"].squeeze()\n",
    "        disc_src_mask = src_disc[\"attention_mask\"].squeeze()\n",
    "        disc_tgt_ids = tgt_disc[\"input_ids\"].squeeze()\n",
    "        disc_tgt_mask = tgt_disc[\"attention_mask\"].squeeze()\n",
    "        \n",
    "        # Mark padding tokens as ignored in the loss\n",
    "        labels[labels == self.tokenizer.pad_token_id] = -100\n",
    "        \n",
    "        return {\n",
    "            # Generator inputs\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": labels,\n",
    "            \n",
    "            # Original source text\n",
    "            \"src_raw\": src,\n",
    "            \n",
    "            # Discriminator inputs\n",
    "            \"disc_src_ids\": disc_src_ids,\n",
    "            \"disc_src_mask\": disc_src_mask,\n",
    "            \"disc_tgt_ids\": disc_tgt_ids,\n",
    "            \"disc_tgt_mask\": disc_tgt_mask\n",
    "        }\n",
    "\n",
    "# Generator (Seq2Seq model)\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, model_name=\"facebook/bart-base\"):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels,\n",
    "            return_dict=True\n",
    "        )\n",
    "        return outputs\n",
    "    \n",
    "    def generate(self, input_ids, attention_mask, **kwargs):\n",
    "        return self.model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "# Discriminator (Classifier)\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, model_name=\"bert-base-uncased\"):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(model_name)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(768*2, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(512, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, src_ids, src_mask, tgt_ids, tgt_mask):\n",
    "        # Encode source text\n",
    "        src_outputs = self.bert(\n",
    "            input_ids=src_ids,\n",
    "            attention_mask=src_mask,\n",
    "            return_dict=True\n",
    "        )\n",
    "        src_pooled = src_outputs.pooler_output\n",
    "        \n",
    "        # Encode target text\n",
    "        tgt_outputs = self.bert(\n",
    "            input_ids=tgt_ids,\n",
    "            attention_mask=tgt_mask,\n",
    "            return_dict=True\n",
    "        )\n",
    "        tgt_pooled = tgt_outputs.pooler_output\n",
    "        \n",
    "        # Concatenate the pooled outputs\n",
    "        combined = torch.cat([src_pooled, tgt_pooled], dim=1)\n",
    "        \n",
    "        # Classify whether the target is real (1) or fake (0)\n",
    "        return self.classifier(combined)\n",
    "        \n",
    "# Evaluation function with BLEU scoring\n",
    "def evaluate_model(generator, discriminator, dataloader, device, gen_only=False):\n",
    "    generator.eval()\n",
    "    if not gen_only:\n",
    "        discriminator.eval()\n",
    "    \n",
    "    g_loss = 0\n",
    "    d_loss = 0\n",
    "    disc_real_acc = 0\n",
    "    disc_fake_acc = 0\n",
    "    all_bleu_scores = []\n",
    "    \n",
    "    # Initialize smoothing function for BLEU\n",
    "    smoothie = SmoothingFunction().method1\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            # Generator evaluation\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            \n",
    "            g_outputs = generator(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "            \n",
    "            g_loss += g_outputs.loss.item()\n",
    "            \n",
    "            # Generate text for BLEU scoring and discriminator eval\n",
    "            gen_ids = generator.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                max_length=128,\n",
    "                num_beams=4,\n",
    "                early_stopping=True\n",
    "            )\n",
    "            \n",
    "            # Skip discriminator evaluation if only evaluating generator\n",
    "            if gen_only:\n",
    "                continue\n",
    "                \n",
    "            # Prepare inputs for discriminator\n",
    "            disc_src_ids = batch[\"disc_src_ids\"].to(device)\n",
    "            disc_src_mask = batch[\"disc_src_mask\"].to(device)\n",
    "            disc_tgt_ids = batch[\"disc_tgt_ids\"].to(device)\n",
    "            disc_tgt_mask = batch[\"disc_tgt_mask\"].to(device)\n",
    "            \n",
    "            # Tokenize generated text for discriminator\n",
    "            gen_text = dataloader.dataset.tokenizer.batch_decode(gen_ids, skip_special_tokens=True)\n",
    "            gen_encoded = dataloader.dataset.bert_tokenizer(\n",
    "                gen_text,\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                max_length=128,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            gen_ids = gen_encoded[\"input_ids\"].to(device)\n",
    "            gen_mask = gen_encoded[\"attention_mask\"].to(device)\n",
    "            \n",
    "            # Real samples classification\n",
    "            real_preds = discriminator(disc_src_ids, disc_src_mask, disc_tgt_ids, disc_tgt_mask)\n",
    "            real_labels = torch.ones_like(real_preds).to(device)\n",
    "            \n",
    "            # Fake samples classification\n",
    "            fake_preds = discriminator(disc_src_ids, disc_src_mask, gen_ids, gen_mask)\n",
    "            fake_labels = torch.zeros_like(fake_preds).to(device)\n",
    "            \n",
    "            # Calculate discriminator loss and accuracy\n",
    "            d_loss_real = nn.BCELoss()(real_preds, real_labels)\n",
    "            d_loss_fake = nn.BCELoss()(fake_preds, fake_labels)\n",
    "            d_loss += (d_loss_real + d_loss_fake).item() / 2\n",
    "            \n",
    "            # Calculate accuracies\n",
    "            disc_real_acc += ((real_preds > 0.5).float() == real_labels).float().mean().item()\n",
    "            disc_fake_acc += ((fake_preds > 0.5).float() == fake_labels).float().mean().item()\n",
    "            \n",
    "            # Calculate BLEU scores\n",
    "            for ref, pred in zip(batch[\"labels\"], gen_ids):\n",
    "                ref_tokens = dataloader.dataset.tokenizer.decode(\n",
    "                    ref[ref != -100], skip_special_tokens=True\n",
    "                ).lower().split()\n",
    "                \n",
    "                pred_tokens = dataloader.dataset.tokenizer.decode(\n",
    "                    pred, skip_special_tokens=True\n",
    "                ).lower().split()\n",
    "                \n",
    "                if pred_tokens:  # Avoid empty predictions\n",
    "                    weights = (0.5, 0.3, 0.15, 0.05)  # Focus on lower n-grams\n",
    "                    bleu = sentence_bleu(\n",
    "                        [ref_tokens], \n",
    "                        pred_tokens, \n",
    "                        weights=weights,\n",
    "                        smoothing_function=smoothie\n",
    "                    )\n",
    "                    all_bleu_scores.append(bleu)\n",
    "    \n",
    "    # Calculate averages\n",
    "    avg_g_loss = g_loss / len(dataloader)\n",
    "    \n",
    "    if not gen_only:\n",
    "        avg_d_loss = d_loss / len(dataloader)\n",
    "        avg_real_acc = disc_real_acc / len(dataloader)\n",
    "        avg_fake_acc = disc_fake_acc / len(dataloader)\n",
    "    else:\n",
    "        avg_d_loss = 0\n",
    "        avg_real_acc = 0\n",
    "        avg_fake_acc = 0\n",
    "        \n",
    "    avg_bleu = np.mean(all_bleu_scores) if all_bleu_scores else 0\n",
    "    \n",
    "    return {\n",
    "        \"g_loss\": avg_g_loss,\n",
    "        \"d_loss\": avg_d_loss,\n",
    "        \"real_acc\": avg_real_acc,\n",
    "        \"fake_acc\": avg_fake_acc,\n",
    "        \"bleu\": avg_bleu\n",
    "    }\n",
    "\n",
    "# GAN Training Function\n",
    "def train_gan(\n",
    "    csv_path,\n",
    "    output_dir=\"sarcasm_gan_model\",\n",
    "    gen_model_name=\"facebook/bart-base\",\n",
    "    disc_model_name=\"bert-base-uncased\",\n",
    "    batch_size=8,\n",
    "    epochs=5,\n",
    "    g_lr=5e-5,\n",
    "    d_lr=1e-5,\n",
    "    max_length=128,\n",
    "    eval_steps=50,\n",
    "    prompt_style=\"random\",\n",
    "    g_steps=1,  # How many generator steps per discriminator step\n",
    "    d_steps=1,  # How many discriminator steps per generator step\n",
    "    gan_weight=0.5  # Weight for GAN loss vs. supervised loss\n",
    "):\n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Initialize tokenizers\n",
    "    gen_tokenizer = AutoTokenizer.from_pretrained(gen_model_name)\n",
    "    disc_tokenizer = BertTokenizer.from_pretrained(disc_model_name)\n",
    "    \n",
    "    # Initialize models\n",
    "    generator = Generator(gen_model_name).to(device)\n",
    "    discriminator = Discriminator(disc_model_name).to(device)\n",
    "    \n",
    "    # Split data for validation\n",
    "    df = pd.read_csv(csv_path)\n",
    "    train_df, val_df = train_test_split(df, test_size=0.1, random_state=42)\n",
    "    \n",
    "    # Save splits temporarily\n",
    "    train_path = \"train_temp.csv\"\n",
    "    val_path = \"val_temp.csv\"\n",
    "    train_df.to_csv(train_path, index=False)\n",
    "    val_df.to_csv(val_path, index=False)\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = SarcasmGANDataset(train_path, gen_tokenizer, disc_tokenizer, max_length, prompt_style)\n",
    "    val_dataset = SarcasmGANDataset(val_path, gen_tokenizer, disc_tokenizer, max_length, \"instruction\")\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    \n",
    "    # Optimizers\n",
    "    g_optimizer = torch.optim.AdamW(generator.parameters(), lr=g_lr, weight_decay=0.01)\n",
    "    d_optimizer = torch.optim.AdamW(discriminator.parameters(), lr=d_lr, weight_decay=0.01)\n",
    "    \n",
    "    # For tracking metrics\n",
    "    best_bleu = 0\n",
    "    step_counter = 0\n",
    "    history = {\n",
    "        'g_loss': [],\n",
    "        'd_loss': [],\n",
    "        'real_acc': [],\n",
    "        'fake_acc': [],\n",
    "        'bleu': [],\n",
    "        'val_g_loss': [],\n",
    "        'val_d_loss': [],\n",
    "        'val_bleu': []\n",
    "    }\n",
    "    \n",
    "    # BCE Loss for discriminator\n",
    "    bce_loss = nn.BCELoss()\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        generator.train()\n",
    "        discriminator.train()\n",
    "        \n",
    "        epoch_g_loss = 0\n",
    "        epoch_d_loss = 0\n",
    "        epoch_gan_loss = 0\n",
    "        \n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "        \n",
    "        for batch in progress_bar:\n",
    "            # Get batch data\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            \n",
    "            disc_src_ids = batch[\"disc_src_ids\"].to(device)\n",
    "            disc_src_mask = batch[\"disc_src_mask\"].to(device)\n",
    "            disc_tgt_ids = batch[\"disc_tgt_ids\"].to(device)\n",
    "            disc_tgt_mask = batch[\"disc_tgt_mask\"].to(device)\n",
    "            \n",
    "            # ------ Train Generator ------\n",
    "            for _ in range(g_steps):\n",
    "                g_optimizer.zero_grad()\n",
    "                \n",
    "                # Supervised loss\n",
    "                g_outputs = generator(input_ids, attention_mask, labels)\n",
    "                g_supervised_loss = g_outputs.loss\n",
    "                \n",
    "                # Generate text for adversarial loss\n",
    "                with torch.no_grad():\n",
    "                    gen_ids = generator.generate(\n",
    "                        input_ids=input_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                        max_length=max_length,\n",
    "                        num_beams=4,\n",
    "                        early_stopping=True\n",
    "                    )\n",
    "                \n",
    "                # Tokenize generated text for discriminator\n",
    "                gen_text = gen_tokenizer.batch_decode(gen_ids, skip_special_tokens=True)\n",
    "                gen_encoded = disc_tokenizer(\n",
    "                    gen_text,\n",
    "                    padding=\"max_length\",\n",
    "                    truncation=True,\n",
    "                    max_length=max_length,\n",
    "                    return_tensors=\"pt\"\n",
    "                )\n",
    "                gen_ids_disc = gen_encoded[\"input_ids\"].to(device)\n",
    "                gen_mask_disc = gen_encoded[\"attention_mask\"].to(device)\n",
    "                \n",
    "                # Adversarial loss (fool the discriminator)\n",
    "                fake_preds = discriminator(disc_src_ids, disc_src_mask, gen_ids_disc, gen_mask_disc)\n",
    "                g_gan_loss = bce_loss(fake_preds, torch.ones_like(fake_preds).to(device))\n",
    "                \n",
    "                # Combined loss\n",
    "                g_loss = g_supervised_loss + gan_weight * g_gan_loss\n",
    "                g_loss.backward()\n",
    "                \n",
    "                # Apply gradient clipping\n",
    "                torch.nn.utils.clip_grad_norm_(generator.parameters(), 1.0)\n",
    "                g_optimizer.step()\n",
    "                \n",
    "                epoch_g_loss += g_supervised_loss.item()\n",
    "                epoch_gan_loss += g_gan_loss.item()\n",
    "            \n",
    "            # ------ Train Discriminator ------\n",
    "            for _ in range(d_steps):\n",
    "                d_optimizer.zero_grad()\n",
    "                \n",
    "                # Generate new text (with detached generator)\n",
    "                with torch.no_grad():\n",
    "                    gen_ids = generator.generate(\n",
    "                        input_ids=input_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                        max_length=max_length,\n",
    "                        num_beams=4,\n",
    "                        early_stopping=True\n",
    "                    )\n",
    "                \n",
    "                # Tokenize generated text for discriminator\n",
    "                gen_text = gen_tokenizer.batch_decode(gen_ids, skip_special_tokens=True)\n",
    "                gen_encoded = disc_tokenizer(\n",
    "                    gen_text,\n",
    "                    padding=\"max_length\",\n",
    "                    truncation=True,\n",
    "                    max_length=max_length,\n",
    "                    return_tensors=\"pt\"\n",
    "                )\n",
    "                gen_ids_disc = gen_encoded[\"input_ids\"].to(device)\n",
    "                gen_mask_disc = gen_encoded[\"attention_mask\"].to(device)\n",
    "                \n",
    "                # Real samples classification\n",
    "                real_preds = discriminator(disc_src_ids, disc_src_mask, disc_tgt_ids, disc_tgt_mask)\n",
    "                real_labels = torch.ones_like(real_preds).to(device)\n",
    "                d_real_loss = bce_loss(real_preds, real_labels)\n",
    "                \n",
    "                # Fake samples classification\n",
    "                fake_preds = discriminator(disc_src_ids, disc_src_mask, gen_ids_disc, gen_mask_disc)\n",
    "                fake_labels = torch.zeros_like(fake_preds).to(device)\n",
    "                d_fake_loss = bce_loss(fake_preds, fake_labels)\n",
    "                \n",
    "                # Combined loss\n",
    "                d_loss = (d_real_loss + d_fake_loss) / 2\n",
    "                d_loss.backward()\n",
    "                \n",
    "                # Apply gradient clipping\n",
    "                torch.nn.utils.clip_grad_norm_(discriminator.parameters(), 1.0)\n",
    "                d_optimizer.step()\n",
    "                \n",
    "                epoch_d_loss += d_loss.item()\n",
    "            \n",
    "            # Update progress bar\n",
    "            progress_bar.set_postfix({\n",
    "                \"G Loss\": g_loss.item(),\n",
    "                \"D Loss\": d_loss.item(),\n",
    "                \"GAN Loss\": g_gan_loss.item()\n",
    "            })\n",
    "            \n",
    "                \n",
    "        # End of epoch evaluation\n",
    "        eval_metrics = evaluate_model(generator, discriminator, val_loader, device)\n",
    "        \n",
    "        # Calculate epoch averages\n",
    "        avg_g_loss = epoch_g_loss / len(train_loader)\n",
    "        avg_d_loss = epoch_d_loss / len(train_loader)\n",
    "        avg_gan_loss = epoch_gan_loss / len(train_loader)\n",
    "        \n",
    "        # Update history\n",
    "        history['g_loss'].append(avg_g_loss)\n",
    "        history['d_loss'].append(avg_d_loss)\n",
    "        history['val_g_loss'].append(eval_metrics['g_loss'])\n",
    "        history['val_d_loss'].append(eval_metrics['d_loss'])\n",
    "        history['real_acc'].append(eval_metrics['real_acc'])\n",
    "        history['fake_acc'].append(eval_metrics['fake_acc'])\n",
    "        history['bleu'].append(eval_metrics['bleu'])\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch+1} Summary:\")\n",
    "        print(f\"  Train G Loss: {avg_g_loss:.4f}, D Loss: {avg_d_loss:.4f}, GAN Loss: {avg_gan_loss:.4f}\")\n",
    "        print(f\"  Val G Loss: {eval_metrics['g_loss']:.4f}, D Loss: {eval_metrics['d_loss']:.4f}\")\n",
    "        print(f\"  Val D Real Acc: {eval_metrics['real_acc']:.4f}, Fake Acc: {eval_metrics['fake_acc']:.4f}\")\n",
    "        print(f\"  Val BLEU: {eval_metrics['bleu']:.4f}\")\n",
    "        \n",
    "        # Save checkpoint\n",
    "        checkpoint_dir = os.path.join(output_dir, f\"checkpoint-epoch-{epoch+1}\")\n",
    "        os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "        \n",
    "        # Save generator\n",
    "        generator.model.save_pretrained(os.path.join(checkpoint_dir, \"generator\"))\n",
    "        gen_tokenizer.save_pretrained(os.path.join(checkpoint_dir, \"generator\"))\n",
    "        \n",
    "        # Save discriminator\n",
    "        torch.save(discriminator.state_dict(), os.path.join(checkpoint_dir, \"discriminator.pt\"))\n",
    "    \n",
    "    # Clean up temp files\n",
    "    if os.path.exists(train_path):\n",
    "        os.remove(train_path)\n",
    "    if os.path.exists(val_path):\n",
    "        os.remove(val_path)\n",
    "    \n",
    "    # Save final models\n",
    "    generator.model.save_pretrained(os.path.join(output_dir, \"generator\"))\n",
    "    gen_tokenizer.save_pretrained(os.path.join(output_dir, \"generator\"))\n",
    "    torch.save(discriminator.state_dict(), os.path.join(output_dir, \"discriminator.pt\"))\n",
    "    \n",
    "    # Save history as numpy arrays\n",
    "    np.save(os.path.join(output_dir, \"training_history.npy\"), history)\n",
    "    \n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(history['g_loss'], label='Train G Loss')\n",
    "    plt.plot(history['val_g_loss'], label='Val G Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Generator Loss')\n",
    "    \n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(history['d_loss'], label='Train D Loss')\n",
    "    plt.plot(history['val_d_loss'], label='Val D Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Discriminator Loss')\n",
    "    \n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(history['real_acc'], label='Real Accuracy')\n",
    "    plt.plot(history['fake_acc'], label='Fake Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.title('Discriminator Accuracy')\n",
    "    \n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.plot(history['bleu'], label='BLEU Score')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('BLEU')\n",
    "    plt.title('BLEU Score')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, \"training_history.png\"))\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Training complete. Models saved to {output_dir}\")\n",
    "    return generator, discriminator, gen_tokenizer, history\n",
    "\n",
    "# Similarity helpers\n",
    "def jaccard_similarity(text1, text2):\n",
    "    words1 = set(text1.lower().split())\n",
    "    words2 = set(text2.lower().split())\n",
    "    \n",
    "    intersection = words1.intersection(words2)\n",
    "    union = words1.union(words2)\n",
    "    \n",
    "    return len(intersection) / max(1, len(union))\n",
    "\n",
    "def get_best_output(original_text, candidates):\n",
    "    \"\"\"Select the best output from multiple candidates\"\"\"\n",
    "    if not candidates:\n",
    "        return f\"I meant: {original_text}\"\n",
    "    \n",
    "    # Original text metrics\n",
    "    orig_len = len(original_text.split())\n",
    "    \n",
    "    scores = []\n",
    "    for output in candidates:\n",
    "        if not output or not output.strip():\n",
    "            scores.append(0)\n",
    "            continue\n",
    "            \n",
    "        output_len = len(output.split())\n",
    "        \n",
    "        # Score based on length ratio (prefer similar length to original)\n",
    "        len_ratio = min(output_len / max(1, orig_len), orig_len / max(1, output_len))\n",
    "        \n",
    "        # Check if output is too similar to input (avoid minimal changes)\n",
    "        similarity = jaccard_similarity(original_text, output)\n",
    "        \n",
    "        # Penalize outputs that are too similar or too different\n",
    "        score = len_ratio * (1 - abs(similarity - 0.3))\n",
    "        scores.append(score)\n",
    "    \n",
    "    # Return the candidate with highest score\n",
    "    if not scores or max(scores) == 0:\n",
    "        return f\"I meant: {original_text}\"\n",
    "    \n",
    "    best_idx = np.argmax(scores)\n",
    "    return candidates[best_idx]\n",
    "\n",
    "# Enhanced GAN-based inference function\n",
    "def rewrite_sarcasm_gan(\n",
    "    text, \n",
    "    gen_model_dir=None, \n",
    "    disc_model_dir=None,\n",
    "    generator=None, \n",
    "    discriminator=None,\n",
    "    gen_tokenizer=None, \n",
    "    disc_tokenizer=None,\n",
    "    ensemble=True  # Whether to use prompt ensemble\n",
    "):\n",
    "    if generator is None or gen_tokenizer is None:\n",
    "        try:\n",
    "            # Try to load as local path by adding \"local-\" prefix\n",
    "            gen_tokenizer = AutoTokenizer.from_pretrained(gen_model_dir, local_files_only=True)\n",
    "            # For Generator, we need to load the model correctly\n",
    "            generator = Generator()\n",
    "            generator.model = AutoModelForSeq2SeqLM.from_pretrained(gen_model_dir, local_files_only=True).to(device)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading generator: {e}\")\n",
    "            raise\n",
    "    \n",
    "    if discriminator is None or disc_tokenizer is None:\n",
    "        if disc_model_dir:  # Only load discriminator if path is provided\n",
    "            try:\n",
    "                disc_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "                discriminator = Discriminator(\"bert-base-uncased\").to(device)\n",
    "                # Load discriminator weights from local file\n",
    "                discriminator.load_state_dict(torch.load(os.path.join(disc_model_dir, \"discriminator.pt\")))\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading discriminator: {e}\")\n",
    "                # Continue without discriminator if loading fails\n",
    "                discriminator = None\n",
    "                disc_tokenizer = None\n",
    "    \n",
    "    generator.eval()\n",
    "    if discriminator:\n",
    "        discriminator.eval()\n",
    "    # Use ensemble approach if enabled\n",
    "    if ensemble:\n",
    "        prompts = [\n",
    "            f\"Remove sarcasm from the following text: {text}\",\n",
    "            f\"Rewrite without sarcasm: {text}\",\n",
    "            f\"Transform this sarcastic statement into a non-sarcastic one: {text}\",\n",
    "            f\"Make this statement neutral and straightforward: {text}\"\n",
    "        ]\n",
    "    else:\n",
    "        prompts = [f\"Remove sarcasm from the following text: {text}\"]\n",
    "    \n",
    "    all_outputs = []\n",
    "    disc_scores = []\n",
    "    \n",
    "    # Generate with multiple prompts\n",
    "    for prompt in prompts:\n",
    "        try:\n",
    "            # Tokenize prompt\n",
    "            src_encoded = gen_tokenizer(\n",
    "                prompt, \n",
    "                return_tensors=\"pt\", \n",
    "                max_length=128, \n",
    "                truncation=True\n",
    "            ).to(device)\n",
    "            \n",
    "            # Generate output\n",
    "            with torch.no_grad():\n",
    "                gen_ids = generator.generate(\n",
    "                    input_ids=src_encoded[\"input_ids\"],\n",
    "                    attention_mask=src_encoded[\"attention_mask\"],\n",
    "                    max_length=128,\n",
    "                    num_beams=5,\n",
    "                    min_length=4,\n",
    "                    length_penalty=1.0,\n",
    "                    no_repeat_ngram_size=3,\n",
    "                    early_stopping=True,\n",
    "                    do_sample=False\n",
    "                )\n",
    "            \n",
    "            # Decode output\n",
    "            output = gen_tokenizer.decode(gen_ids[0], skip_special_tokens=True)\n",
    "            \n",
    "            # Only add non-empty outputs\n",
    "            if output and len(output.strip()) > 0:\n",
    "                all_outputs.append(output)\n",
    "                \n",
    "                # Score with discriminator if available\n",
    "                if discriminator:\n",
    "                    with torch.no_grad():\n",
    "                        # Tokenize for discriminator\n",
    "                        src_disc = disc_tokenizer(\n",
    "                            text,\n",
    "                            return_tensors=\"pt\",\n",
    "                            max_length=128,\n",
    "                            padding=\"max_length\",\n",
    "                            truncation=True\n",
    "                        ).to(device)\n",
    "                        \n",
    "                        tgt_disc = disc_tokenizer(\n",
    "                            output,\n",
    "                            return_tensors=\"pt\",\n",
    "                            max_length=128,\n",
    "                            padding=\"max_length\",\n",
    "                            truncation=True\n",
    "                        ).to(device)\n",
    "                        \n",
    "                        # Get discriminator score (higher is better - more realistic)\n",
    "                        disc_score = discriminator(\n",
    "                            src_disc[\"input_ids\"], \n",
    "                            src_disc[\"attention_mask\"],\n",
    "                            tgt_disc[\"input_ids\"],\n",
    "                            tgt_disc[\"attention_mask\"]\n",
    "                        ).item()\n",
    "                        \n",
    "                        disc_scores.append(disc_score)\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating with prompt '{prompt}': {e}\")\n",
    "    \n",
    "    # If no outputs, try fallback\n",
    "    if not all_outputs:\n",
    "        try:\n",
    "            # Fallback to direct generation with no frills\n",
    "            input_text = f\"Make this non-sarcastic: {text}\"\n",
    "            input_ids = gen_tokenizer(input_text, return_tensors=\"pt\").input_ids.to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                output_ids = generator.generate(\n",
    "                    input_ids,\n",
    "                    max_length=128,\n",
    "                    num_beams=4,\n",
    "                    do_sample=False,\n",
    "                    early_stopping=True\n",
    "                )\n",
    "            \n",
    "            fallback_output = gen_tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "            if fallback_output and len(fallback_output.strip()) > 0:\n",
    "                return fallback_output\n",
    "            else:\n",
    "                return f\"I meant: {text}\"\n",
    "        except:\n",
    "            return f\"I meant: {text}\"\n",
    "    \n",
    "    # If we have discriminator scores, use them to pick the best\n",
    "    if discriminator and disc_scores:\n",
    "        best_idx = np.argmax(disc_scores)\n",
    "        return all_outputs[best_idx]\n",
    "    \n",
    "    # Otherwise use our heuristic selection\n",
    "    return get_best_output(text, all_outputs)\n",
    "\n",
    "# Process multiple examples\n",
    "def process_examples(texts, gen_model_dir=None, disc_model_dir=None):\n",
    "    # Load models once\n",
    "    gen_tokenizer = AutoTokenizer.from_pretrained(gen_model_dir)\n",
    "    generator = Generator()\n",
    "    generator.model = AutoModelForSeq2SeqLM.from_pretrained(gen_model_dir).to(device)\n",
    "    \n",
    "    # Try to load discriminator if available\n",
    "    try:\n",
    "        disc_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "        discriminator = Discriminator().to(device)\n",
    "        discriminator.load_state_dict(torch.load(os.path.join(disc_model_dir, \"discriminator.pt\")))\n",
    "    except:\n",
    "        discriminator = None\n",
    "        disc_tokenizer = None\n",
    "    \n",
    "    results = []\n",
    "    for text in tqdm(texts, desc=\"Processing texts\"):\n",
    "        rewritten = rewrite_sarcasm_gan(\n",
    "            text, \n",
    "            generator=generator, \n",
    "            discriminator=discriminator,\n",
    "            gen_tokenizer=gen_tokenizer, \n",
    "            disc_tokenizer=disc_tokenizer\n",
    "        )\n",
    "        results.append((text, rewritten))\n",
    "    \n",
    "    return pd.DataFrame(results, columns=['Original', 'Rewritten'])\n",
    "\n",
    "# Example usage - you can uncomment and run directly\n",
    "\n",
    "# 1. Train the model\n",
    "# csv_path = \"sarcasm_data.csv\"\n",
    "# output_dir = \"sarcasm_gan_model\"\n",
    "# generator, discriminator, tokenizer, history = train_gan(\n",
    "#     csv_path=csv_path,\n",
    "#     output_dir=output_dir,\n",
    "#     epochs=15,\n",
    "#     batch_size=16\n",
    "# )\n",
    "\n",
    "# 2. Inference with a single example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewritten: There is no need to ignore all evidence and cling to my flawless reasoning.\n"
     ]
    }
   ],
   "source": [
    "rewritten = rewrite_sarcasm_gan(\n",
    "    \"Sure, let’s ignore all evidence and cling to your flawless reasoning\",\n",
    "    gen_model_dir=\"sarcasm_gan_model/generator\",  # Use this path instead of best_generator\n",
    "    disc_model_dir=\"sarcasm_gan_model\" \n",
    ")\n",
    "\n",
    "print(f\"Rewritten: {rewritten}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewritten: I don’t need your approval, I have my own opinion. \n"
     ]
    }
   ],
   "source": [
    "# Copyright 2025 Umang Patel\n",
    "# \n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "# \n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "# \n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "\n",
    "rewritten = rewrite_sarcasm_gan(\n",
    "    \"I don’t need your approval, darling, I have my own\",\n",
    "    gen_model_dir=\"sarcasm_gan_model/generator\",  # Use this path instead of best_generator\n",
    "    disc_model_dir=\"sarcasm_gan_model\" \n",
    ")\n",
    "\n",
    "print(f\"Rewritten: {rewritten}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewritten: Having to arrive exactly three minutes late is disappointing but not disappointing.\n"
     ]
    }
   ],
   "source": [
    "rewritten = rewrite_sarcasm_gan(\n",
    "    \"Congratulations on arriving exactly three minutes late Your punctuality is truly inspiring\",\n",
    "    gen_model_dir=\"sarcasm_gan_model/generator\",  # Use this path instead of best_generator\n",
    "    disc_model_dir=\"sarcasm_gan_model\" \n",
    ")\n",
    "\n",
    "print(f\"Rewritten: {rewritten}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewritten: It's not nice of you to show up three minutes late. \n"
     ]
    }
   ],
   "source": [
    "rewritten = rewrite_sarcasm_gan(\n",
    "    \"Nice of you to show up three minutes late your timing really is something else\",\n",
    "    gen_model_dir=\"sarcasm_gan_model/generator\",  # Use this path instead of best_generator\n",
    "    disc_model_dir=\"sarcasm_gan_model\" \n",
    ")\n",
    "\n",
    "print(f\"Rewritten: {rewritten}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewritten: There is no need to stating the obvious. \n"
     ]
    }
   ],
   "source": [
    "rewritten = rewrite_sarcasm_gan(\n",
    "    \"Congratulations on stating the obvious. I’m sure glaciers will start moving any minute now\",\n",
    "    gen_model_dir=\"sarcasm_gan_model/generator\",  # Use this path instead of best_generator\n",
    "    disc_model_dir=\"sarcasm_gan_model\" \n",
    ")\n",
    "\n",
    "print(f\"Rewritten: {rewritten}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewritten: There are a lot of things I don't like doing. \n"
     ]
    }
   ],
   "source": [
    "rewritten = rewrite_sarcasm_gan(\n",
    "    \"Absolutely, let’s add that to the dozen other things I definitely wasn’t planning to do.\",\n",
    "    gen_model_dir=\"sarcasm_gan_model/generator\",  # Use this path instead of best_generator\n",
    "    disc_model_dir=\"sarcasm_gan_model\" \n",
    ")\n",
    "\n",
    "print(f\"Rewritten: {rewritten}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
